{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKpnmVCp1Luw50p/jKsZiP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smjenski-izvjestaji/izvjestaji/blob/main/source_kod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "1vsVZYiqhzk4",
        "outputId": "dd8f65e6-14b0-40fa-c844-6284e4d0cf71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Processing line: RGB (Station ID: 1)\n",
            "✅ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4122486230.py:565: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-4122486230.py:566: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_22fa3d67-c541-4758-89a2-cf0b8e649476\", \"updated_shift_report.xlsx\", 10726)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: PET1 (Station ID: 2)\n",
            "✅ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4122486230.py:565: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-4122486230.py:566: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9421e2dd-a28c-4b49-917c-54fd9e92c117\", \"updated_shift_report.xlsx\", 10676)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: PET2 (Station ID: 3)\n",
            "✅ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4122486230.py:565: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-4122486230.py:566: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aee388c6-a54e-4b63-8a2c-727e912292b3\", \"updated_shift_report.xlsx\", 10687)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install pandas openpyxl requests\n",
        "\n",
        "# Step 2: Upload the Excel template\n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import time as systime\n",
        "import ast\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.cell.cell import MergedCell\n",
        "from openpyxl.utils import get_column_letter\n",
        "from datetime import datetime, time\n",
        "from openpyxl.styles import Alignment, Font, Border  # remove when going back to .py version\n",
        "from copy import copy\n",
        "\n",
        "import requests\n",
        "\n",
        "from datetime import datetime, time\n",
        "from openpyxl import Workbook\n",
        "\n",
        "from datetime import datetime, time, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "\n",
        "\n",
        "import openpyxl\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "\n",
        "#from google.colab import files\n",
        "\n",
        "dict_stop_reasons={\"Electrical Failure\":\"Električni kvar\", \"Bottle Necks\":\"Usko grlo\", \"COP\":\"COP\", \"End of production\":\"Završetak proizvodnje\", \"Not recorded\":\"Nije zabilježen\", \"Preventive Maintenance\":\"Preventivno održavanje\", \"Changeover\":\"Changeover\", \"Other failure\":\"Drugi kvar\", \"Adjustments on equipment\":\"Prilagodbe na opremi\", \"CIP (5ST)\":\"CIP (5ST)\", \"Mechanical failure\":\"Mehanički kvar\",\"Meeting\":\"Sastanak\", \"Start of production\":\"Početak proizvodnje\", \"CIP (3V)\":\"CIP (3V)\", \"Cleaning on line\":\"Čišćenje na liniji\", \"CIP (3L)\":\"CIP (3L)\", \"Insufficient Electrical en.supply\":\"Nedovoljna opskrba elektr. energije\",\"CIP (OV)\":\"CIP (OV)\",\"Insufficient Water supply\":\"Nedovoljna opskrba vode\",\"Lack of personnel\":\"Nedostatak operatera\",\"Changeover + CIP (3V)\":\"Changeover + CIP (3V)\",\"Not skilled personnel\":\"Nekvalificirani operateri\",\"Changeover + CIP (3L)\":\"Changeover + CIP (3L)\",\"Other\":\"Drugo\",\"Changeover + CIP (OV)\":\"Changeover + CIP (OV)\",\"QA tests on the line\":\"QA testovi na liniji\",\"Test bottle\":\"Probna boca\",\"Falling bottles on conveyors\":\"Padaju boce na transporterima\",\"P1 (CHO 12/24)\":\"P1 (CHO 12/24)\",\"Stopped production from QA\":\"Zaustavljena proizvodnja iz QA\",\"P2 (CHO filler)\":\"P2 (CHO filler)\",\"Insufficient CO2 supply\":\"Nedovoljna opskrba CO2\",\"P3 (CHO+OV+MIX)\":\"P3 (CHO+OV+MIX)\",\"Waiting for the Syrup\":\"Čekanje sirupa\",\"P4 (CHO+3°+MIX)\":\"P4 (CHO+3°+MIX)\",\"Insufficient Cold water supply\":\"Nedovoljna opskrba hladne vode\",\"P5 (CHO+5°+MIX)\":\"P5 (CHO+5°+MIX)\",\"Poor CHO performance\":\"Loš performans changeovera\",\"P6 (OV+MIX)\":\"P6 (OV+MIX)\",\"Poor Bottles Quality\":\"Loša kvaliteta boca\",\"P7 (3°+MIX)\":\"P7 (3°+MIX)\",\"Poor Crates Quality\":\"Loša kvaliteta sanduka\",\"P8 (5°+MIX)\":\"P8 (5°+MIX)\",\"Poor Labels Quality\":\"Loša kvaliteta etiketa\",\"P9 (DZ+OV+MIX)\":\"P9 (DZ+OV+MIX)\",\"Poor Pallets (Wood) Quality\":\"Loša kvaliteta drvenih paleta\",\"P10 (DZ+3°+MIX)\":\"P10 (DZ+3°+MIX)\",\"Poor Cap Quality\":\"Loša kvaliteta zatvarača\",\"P11 (DZ+5°+MIX)\":\"P11 (DZ+5°+MIX)\",\"Poor support from Storage\":\"Loša podrška iz skladišta\",\"Poor lubrication of conveyors\":\"Slabo podmazivanje transportera\",\"Poor thermofoil quality\":\"Loša kvaliteta termofolije\",\"Poor Preform Quality\":\"Loša kvaliteta predforme\",\"Replenishment of raw materials\":\"Nadopuna repromaterijala\",\"Shift break\":\"Smjenska pauza\",\"Mixed crates\":\"Miješani sanduci\",\"Mixed bottles\":\"Miješane boce\",\"Dirty crates\":\"Prljavi sanduci\",\"Dirty bottles\":\"Prljave boce\",\"test\":\"test\",\"Uncommented\":\"Bez komentara\"}\n",
        "dict_stop_type={\"EPL\":\"EPL\",\"OPL\":\"OPL\",\"Non scheduled time\":\"Neplanirano vrijeme\",\"Changeover and CIP\":\"Changeover i CIP\",\"Maintenance and Set up\":\"Održavanje i setup\",\"Minor Stoppages\":\"Manji zastoji\",\"Preventive Maintenance\":\"Preventivno održavanje\",\"Speed Losses\":\"Gubici brzine\",\"Uncommented\":\"Bez komentara\"}\n",
        "dict_speed_loss_reasons={\"Speed loss\":\"Gubitak brzine\"}\n",
        "dict_speed_loss_group={\"Speed Loss\":\"Gubitak brzine\"}\n",
        "dict_locations={\"Infeed of caser\":\"Ulaz u upakivač\",\"Crates conveyors from caser to palletizer\":\"Transporteri sanduka od upakivača do paletizera\",\"Bottles conveyor from uncaser to washer machine\":\"Transporter boca od ispakivača do perilice\",\"Bottle washer\":\"Perilica boca\",\"Bottles conveyor from washer to filler\":\"Transporter boca od perilice do punjača\",\"Linatronic\":\"Linatronic\",\"Mixer\":\"Mixer\",\"Bottles conveyor from filler to labelling machine\":\"Transporter boca od punjača do etiketirke\",\"Bottles conveyor from labelling machine to caser\":\"Transporter boca od etiketirke do upakivača\",\"Caser\":\"Upakivač\",\"Crates conveyors from depalletizer to uncaser\":\"Transporteri sanduka od depaletizera do ispakivača\",\"Uncaser\":\"Ispakivač\",\"Infeed of bottle washer\":\"Ulaz u perilicu boca\",\"Outfeed from washer machine\":\"Izlaz iz perilice boca\",\"Inliner in white line\":\"Inliner u bijeloj liniji\",\"Cap conveyors\":\"Transporteri zatvarača\",\"Checkmat after filler\":\"Checkmat nakon punjača\",\"Depalletizer\":\"Depaletizer\",\"Filler\":\"Punjač\",\"Palletizer\":\"Paletizer\",\"Labeller\":\"Etiketirka\",\"Crates conveyor from uncaser to washer\":\"Transporter sanduka od ispakivača do perilice sanduka\",\"Crates conveyor from washer to caser\":\"Transporter sanduka od perilice sanduka do upakivača\",\"Crates washer\":\"Perilica sanduka\",\"Packer (Zambelli)\":\"Pakiralica (Zambelli)\",\"Blower\":\"Puhalica\",\"Packer (KHS)\":\"Pakiralica (KHS)\",\"Handle machine\":\"Ručkomat\",\"Air conveyor\":\"Zračni transporter\",\"Conveyor from labeller to packer (KHS)\":\"Transporter od etiketirke do pakiralice (KHS)\",\"Conveyor from KHS to handle machine\":\"Transporter od KHS do ručkomata\",\"Conveyor from handle machine to Zambelli\":\"Transporter od ručkomata do Zambelli-a\",\"Conveyor from packer (Zambelli) to palletizer\":\"Transporter od pakiralice (Zambelli) do paletizera\",\"Conveyor from labeller to packer (Zambelli)\":\"Transporter od etiketirke do pakiralice (Zambelli)\",\"Entire line\":\"Cijela linija\",\"Water treatment\":\"Obrada vode\",\"Syrup room\":\"Sirupana\",\"Compressor room\":\"Kompresorska stanica\",\"Cooler\":\"Hladnjak\",\"Rinser\":\"Ispiračica\",\"Capper\":\"Zatvaračica\",\"Date printer\":\"Datumar\",\"Case sticker applicator\":\"Aplikator naljepnica na paket\",\"Pallet sticker applicator\":\"Aplikator naljepnica na paletu\",\"Foam maker\":\"Pjenomat\"}\n",
        "\n",
        "# Station mapping\n",
        "station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "\n",
        "# Replace with your actual timezone (e.g., 'Europe/Belgrade', 'Europe/Zagreb')\n",
        "local_timezone = ZoneInfo('Europe/Zagreb')\n",
        "\n",
        "# Get local time\n",
        "now = datetime.utcnow()\n",
        "now_local = now.replace(tzinfo=ZoneInfo('UTC')).astimezone(local_timezone)\n",
        "\n",
        "current_time = now_local.time()\n",
        "\"\"\"\n",
        "# Determine shift and date based on local time\n",
        "# vrijeme je u UTC+0 by default a ne CEST (UTC+2) PAZI!!!\n",
        "# zato smo uveli now_local\n",
        "if time(5, 30) <= current_time < time(13, 30):\n",
        "    shift = 1\n",
        "    selected_date = now_local.date()\n",
        "elif time(13, 30) <= current_time < time(21, 30):\n",
        "    shift = 2\n",
        "    selected_date = now_local.date()\n",
        "else:\n",
        "    shift = 3\n",
        "    selected_date = (now_local - timedelta(days=1)).date()  # Use previous day\n",
        "\"\"\"\n",
        "\n",
        "# Determine shift and date based on local time\n",
        "# vrijeme je u UTC+0 by default a ne CEST (UTC+2) PAZI!!!\n",
        "# zato smo uveli now_local\n",
        "if time(6, 40) <= current_time < time(14, 40):\n",
        "    shift = 1\n",
        "    selected_date = now_local.date()\n",
        "elif time(14, 40) <= current_time < time(22, 40):\n",
        "    shift = 2\n",
        "    selected_date = now_local.date()\n",
        "else:\n",
        "    shift = 3\n",
        "    selected_date = (now_local - timedelta(days=1)).date()  # Use previous day\n",
        "\n",
        "\n",
        "\n",
        "# Format date for filename\n",
        "date_str = selected_date.strftime('%Y%m%d')\n",
        "\n",
        "#output_dir = Path(\"excel_reports\")\n",
        "#output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Iterate over each production line\n",
        "for line_input, station_id in station_map.items():\n",
        "    print(f\"Processing line: {line_input} (Station ID: {station_id})\")\n",
        "\n",
        "\n",
        "    # GitHub raw URL to the Excel template\n",
        "    template_url = \"https://raw.githubusercontent.com/smjenski-izvjestaji/izvjestaji/main/Shift_report_template.xlsx\"\n",
        "    filename = \"template.xlsx\"\n",
        "\n",
        "    # 📥 Download the file\n",
        "    response = requests.get(template_url)\n",
        "    if response.status_code == 200:\n",
        "      with open(filename, 'wb') as f:\n",
        "          f.write(response.content)\n",
        "      print(f\"✅ Template downloaded: {filename}\")\n",
        "    else:\n",
        "      raise Exception(f\"❌ Failed to download file: {response.status_code}\")\n",
        "\n",
        "    # 🧠 Step 3: Load workbook and sheet\n",
        "    #filename = next(iter(uploaded))\n",
        "    wb = load_workbook(filename)\n",
        "    #systime.sleep(10)  # Optional delay\n",
        "    sheet_814 = wb['Sheet1']\n",
        "\n",
        "    #api_key = os.getenv(\"API_KEY\")\n",
        "    #secret_key = os.getenv(\"SECRET_KEY\")\n",
        "\n",
        "# 🔐 Step 4: Prepare API credentials\n",
        "    api_key = \"EVO99B7A17C6CF2410\"         # 🔁 Replace with your API key\n",
        "    secret_key = \"v2xLnrFQD18WYtyUEykK\"   # 🔁 Replace with your secret key\n",
        "\n",
        "\n",
        "    # 🔐 Step 4: Prepare API credentials\n",
        "    #api_key = \"API_KEY\"         # 🔁 Replace with your API key\n",
        "    #secret_key = \"SECRET_KEY\"   # 🔁 Replace with your secret key\n",
        "\n",
        "    credentials = f\"{api_key}:{secret_key}\"\n",
        "    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n",
        "\n",
        "    headers = {\n",
        "    \"Authorization\": f\"Basic {encoded_credentials}\"\n",
        "    }\n",
        "\n",
        "    station_id = station_map[line_input]\n",
        "\n",
        "    \"\"\"\n",
        "    # 🏭 Ask user for production line\n",
        "    line_input = input(\"Enter production line (PET1, PET2, or RGB): \").strip().upper().replace(\" \", \"\")\n",
        "    station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "\n",
        "    if line_input not in station_map:\n",
        "    raise ValueError(\"Invalid line. Use RGB, PET1, or PET2.\")\n",
        "\n",
        "    station_id = station_map[line_input]\n",
        "\n",
        "    # 📅 Ask user for date\n",
        "    selected_date_input = input(\"Enter the date to retrieve checklists and stops (DD.MM.YYYY): \").strip()\n",
        "    try:\n",
        "        selected_date = datetime.strptime(selected_date_input, \"%d.%m.%Y\").date()\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Invalid date format. Please use DD.MM.YYYY.\")\n",
        "\n",
        "\n",
        "    # 👥 Ask user for shift number\n",
        "    shift_input = input(\"Enter shift number (1, 2, or 3): \").strip()\n",
        "    if shift_input not in ['1', '2', '3']:\n",
        "        raise ValueError(\"Invalid shift. Please enter 1, 2, or 3.\")\n",
        "    shift = int(shift_input)\n",
        "    shift_label = {1: \"1st\", 2: \"2nd\", 3: \"3rd\"}[shift]\n",
        "    \"\"\"\n",
        "\n",
        "    # Station map\n",
        "    station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "    \"\"\"\n",
        "    # 📅 Automatically use today's date\n",
        "    selected_date = datetime.today().date()\n",
        "\n",
        "    # ⏰ Automatically determine shift based on current time\n",
        "    current_time = datetime.now().time()\n",
        "\n",
        "    if time(5, 30) <= current_time < time(13, 30):\n",
        "        shift = 1\n",
        "    elif time(13, 30) <= current_time < time(21, 30):\n",
        "        shift = 2\n",
        "    else:\n",
        "        shift = 3\n",
        "    \"\"\"\n",
        "    shift_label = {1: \"1st\", 2: \"2nd\", 3: \"3rd\"}[shift]\n",
        "\n",
        "\n",
        "    #  Define shift time windows for start and end\n",
        "    def get_shift_bounds(shift, date):\n",
        "        if shift == 1:\n",
        "            return time(5, 30), time(13, 30)\n",
        "        elif shift == 2:\n",
        "            return time(13, 30), time(21, 30)\n",
        "        elif shift == 3:\n",
        "            # Third shift spans 2 dates\n",
        "            return time(21, 30), time(5, 30)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid shift number\")\n",
        "\n",
        "#\n",
        "    # 🧾 Write date and shift to cells D9 and F9\n",
        "    def set_left(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='left')\n",
        "\n",
        "    def set_left_font48(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='left')\n",
        "        cell.font = Font(name=\"Arial\", size=48)\n",
        "\n",
        "    def set_center(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "    def set_right(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='right')\n",
        "\n",
        "    set_left(sheet_814[\"D9\"], selected_date.strftime(\"%d.%m.%Y\"))\n",
        "    set_left(sheet_814[\"F9\"], str(shift))\n",
        "\n",
        "    # write other info in header (punjac, linija, ...)\n",
        "\n",
        "    if line_input == \"PET1\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjač O+H\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE PET BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 866 (PET1)\")\n",
        "\n",
        "    elif line_input == \"PET2\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjač Krones\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE PET BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 814 (PET2)\")\n",
        "\n",
        "    elif line_input == \"RGB\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjač Krones\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE RGB BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 865 (RGB)\")\n",
        "\n",
        "\n",
        "\n",
        "    # 📡 Step 5: Fetch checklist data from API\n",
        "    checklist_url = f\"https://api.evocon.com/api/reports/checklists_json?startTime=2025-03-01&endTime=2050-01-01&stationId={station_id}\"\n",
        "    response = requests.get(checklist_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "      raise Exception(f\"Checklist API error {response.status_code}: {response.text}\")\n",
        "\n",
        "    checklist_df = pd.DataFrame(response.json())\n",
        "\n",
        "    # 🧾 Step 6: Normalize checklist data\n",
        "    def clean_time(t):\n",
        "        if pd.isna(t): return None\n",
        "        t = str(t).replace(\":\", \"\").strip()\n",
        "        return t.zfill(4)[-4:]\n",
        "\n",
        "    # Handle stringified lists in 'itemresult'\n",
        "    def extract_single_value(x):\n",
        "        if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(x)\n",
        "                if isinstance(parsed, list) and len(parsed) == 1:\n",
        "                    return str(parsed[0])\n",
        "            except:\n",
        "                return x\n",
        "        return str(x).strip()\n",
        "\n",
        "    checklist_df['clean_duetime'] = checklist_df['duetime'].apply(clean_time)\n",
        "    checklist_df['duetime_dt'] = pd.to_datetime(checklist_df['duetime'], format='%H:%M', errors='coerce').dt.time\n",
        "    checklist_df['date'] = pd.to_datetime(checklist_df['date'], errors='coerce').dt.date\n",
        "    checklist_df['itemname'] = checklist_df['itemname'].astype(str).str.strip().str.lower()\n",
        "    checklist_df['result'] = checklist_df['result'].astype(str).str.strip().str.lower()\n",
        "    checklist_df['itemresult'] = checklist_df['itemresult'].apply(extract_single_value)\n",
        "\n",
        "    # Apply shift filter\n",
        "    if shift == 1:\n",
        "        time_filter = (checklist_df['date'] == selected_date) & \\\n",
        "                      (checklist_df['duetime_dt'] >= time(6, 0)) & \\\n",
        "                      (checklist_df['duetime_dt'] <= time(14, 0))\n",
        "    elif shift == 2:\n",
        "        time_filter = (checklist_df['date'] == selected_date) & \\\n",
        "                      (checklist_df['duetime_dt'] > time(14, 0)) & \\\n",
        "                      (checklist_df['duetime_dt'] <= time(22, 0))\n",
        "    elif shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        time_filter = (\n",
        "            ((checklist_df['date'] == selected_date) & (checklist_df['duetime_dt'] > time(22, 0))) |\n",
        "            ((checklist_df['date'] == next_day) & (checklist_df['duetime_dt'] <= time(5, 46)))\n",
        "        )\n",
        "\n",
        "    checklist_df = checklist_df[time_filter]\n",
        "\n",
        "    # Map parameter names from D17–D27\n",
        "    param_row_map = {}\n",
        "    for row in range(17, 28):\n",
        "        val = sheet_814[f\"D{row}\"].value\n",
        "        if val:\n",
        "            param_row_map[val.strip().lower()] = row\n",
        "\n",
        "    # Match headers in row 16\n",
        "    header_map = {}\n",
        "    for col in sheet_814.iter_cols(min_row=16, max_row=16):\n",
        "        cell = col[0]\n",
        "        if cell.value:\n",
        "            values = [p.strip().replace(\":\", \"\") for p in str(cell.value).split('/')]\n",
        "            if len(values) == 3:\n",
        "                header_map[col[0].column_letter] = {\n",
        "                    'morning': values[0].zfill(4),\n",
        "                    'afternoon': values[1].zfill(4),\n",
        "                    'night': values[2].zfill(4)\n",
        "                }\n",
        "\n",
        "    # Write checklist values\n",
        "    for _, row in checklist_df.iterrows():\n",
        "        if row['result'] != 'successful':\n",
        "            continue\n",
        "\n",
        "        param_name = row['itemname']\n",
        "        result = row['itemresult']\n",
        "        duetime = row['duetime_dt']\n",
        "        duetime_str = row['clean_duetime']\n",
        "\n",
        "        if pd.isna(duetime) or param_name not in param_row_map:\n",
        "            continue\n",
        "\n",
        "        shift_key = None\n",
        "        if time(6, 15) <= duetime <= time(13, 45):\n",
        "            shift_key = 'morning'\n",
        "        elif time(14, 15) <= duetime <= time(21, 45):\n",
        "            shift_key = 'afternoon'\n",
        "        elif duetime >= time(22, 15) or duetime <= time(5, 45):\n",
        "            shift_key = 'night'\n",
        "\n",
        "        target_column = None\n",
        "        for col_letter, shift_times in header_map.items():\n",
        "            if shift_times.get(shift_key) == duetime_str:\n",
        "                target_column = col_letter\n",
        "                break\n",
        "\n",
        "        if not target_column:\n",
        "            continue\n",
        "\n",
        "        row_number = param_row_map[param_name]\n",
        "        cell = sheet_814[f\"{target_column}{row_number}\"]\n",
        "        if not isinstance(cell, MergedCell):\n",
        "            set_left(cell, result)\n",
        "\n",
        "    # 🌐 API 2: Stop reasons\n",
        "    losses_url = f\"https://api.evocon.com/api/reports/losses_json?stationId={station_id}&startTime=2025-03-01&endTime=2050-01-01\"\n",
        "    response = requests.get(losses_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Losses API error: {response.status_code}\")\n",
        "\n",
        "    losses_df = pd.DataFrame(response.json())\n",
        "\n",
        "    losses_df['shiftStart'] = pd.to_datetime(losses_df['shiftStart'], errors='coerce')\n",
        "    losses_df['start'] = pd.to_datetime(losses_df['start'], errors='coerce')\n",
        "    losses_df['end'] = pd.to_datetime(losses_df['end'], errors='coerce')\n",
        "\n",
        "    filtered_losses = losses_df[\n",
        "        (losses_df['shiftStart'].dt.date == selected_date) &\n",
        "        (losses_df['shiftName'] == shift_label)\n",
        "    ]\n",
        "\n",
        "# Write stops to 814 starting from row 32\n",
        "# New scrit with joined stops\n",
        "\n",
        "    start_row = 32\n",
        "    processed_join_ids = set()\n",
        "\n",
        "    for idx, row in filtered_losses.iterrows():\n",
        "        join_id = row.get('stopJoinId')\n",
        "\n",
        "        if join_id and str(join_id).strip() != \"\":\n",
        "            if join_id in processed_join_ids:\n",
        "                continue\n",
        "            processed_join_ids.add(join_id)\n",
        "\n",
        "            # ✅ get full join group from losses_df (not just filtered)\n",
        "            group = losses_df[losses_df['stopJoinId'] == join_id].copy()\n",
        "            group = group.sort_values(by='start')\n",
        "\n",
        "            total_duration = 0\n",
        "            for _, sub in group.iterrows():\n",
        "                if pd.notna(sub['start']) and pd.notna(sub['end']):\n",
        "                    total_duration += (sub['end'] - sub['start']).total_seconds() / 60\n",
        "            total_duration = round(total_duration, 0)\n",
        "\n",
        "\n",
        "            location = dict_locations.get(group['location'].dropna().astype(str).str.strip().iloc[0], group['location'].dropna().astype(str).str.strip().iloc[0] if not group['location'].dropna().empty else '')\n",
        "            stop_group_val = group['stopGroup'].dropna().astype(str).str.strip().iloc[0] if not group['stopGroup'].dropna().empty else ''\n",
        "            stop_group = dict_stop_type.get(stop_group_val, stop_group_val)\n",
        "            stop_val = str(group['stop'].iloc[0]).strip()\n",
        "            stop = dict_stop_reasons.get(stop_val, stop_val)\n",
        "            comment = str(group['comment'].iloc[0]).strip()\n",
        "            combined = f\"{stop} ({comment})\".strip()\n",
        "\n",
        "            if start_row >= 49:\n",
        "                sheet_814.insert_rows(start_row)\n",
        "\n",
        "                # 🧽 Unmerge any cells that were auto-merged in the inserted row\n",
        "                for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                    if merged_range.min_row == start_row:\n",
        "                        try:\n",
        "                            sheet_814.unmerge_cells(str(merged_range))\n",
        "                        except KeyError:\n",
        "                            pass # merged cell may already have been cleared\n",
        "\n",
        "                # 🎨 Copy style from row 49 to the new row\n",
        "\n",
        "                for col in range(1, sheet_814.max_column + 1):\n",
        "                    source_cell = sheet_814.cell(row=49, column=col)\n",
        "                    target_cell = sheet_814.cell(row=start_row, column=col)\n",
        "\n",
        "                    # Copy formatting if any\n",
        "                    if source_cell.has_style:\n",
        "                        target_cell.font = copy(source_cell.font)\n",
        "                        target_cell.border = copy(source_cell.border)\n",
        "                        target_cell.fill = source_cell.fill\n",
        "                        target_cell.number_format = source_cell.number_format\n",
        "                        target_cell.protection = source_cell.protection\n",
        "                        target_cell.alignment = source_cell.alignment\n",
        "\n",
        "                # Set row height for larger font\n",
        "                sheet_814.row_dimensions[start_row].height = 90\n",
        "\n",
        "\n",
        "            set_left_font48(sheet_814[f\"E{start_row}\"], \"joined stops\")\n",
        "            set_left_font48(sheet_814[f\"F{start_row}\"], \"joined stops\")\n",
        "            set_left_font48(sheet_814[f\"G{start_row}\"], total_duration)\n",
        "            set_left_font48(sheet_814[f\"H{start_row}\"], location)\n",
        "            set_left_font48(sheet_814[f\"K{start_row}\"], stop_group)\n",
        "            set_left_font48(sheet_814[f\"N{start_row}\"], combined)\n",
        "\n",
        "            start_row += 1\n",
        "\n",
        "        else:\n",
        "            # ✅ true individual stop (no join ID or empty)\n",
        "            if start_row >= 49:\n",
        "                sheet_814.insert_rows(start_row)\n",
        "\n",
        "                # 🧽 Unmerge any cells that were auto-merged in the inserted row\n",
        "                for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                    if merged_range.min_row == start_row:\n",
        "                        try:\n",
        "                            sheet_814.unmerge_cells(str(merged_range))\n",
        "                        except KeyError:\n",
        "                            pass # merged cell may already have been cleared\n",
        "\n",
        "            # 🎨 Copy style from row 49 to the new row\n",
        "                for col in range(1, sheet_814.max_column + 1):\n",
        "                    source_cell = sheet_814.cell(row=49, column=col)\n",
        "                    target_cell = sheet_814.cell(row=start_row, column=col)\n",
        "\n",
        "                    # Copy formatting if any\n",
        "                    if source_cell.has_style:\n",
        "                        target_cell.font = copy(source_cell.font)\n",
        "                        target_cell.border = copy(source_cell.border)\n",
        "                        target_cell.fill = copy(source_cell.fill)\n",
        "                        target_cell.number_format = copy(source_cell.number_format)\n",
        "                        target_cell.protection = copy(source_cell.protection)\n",
        "                        target_cell.alignment = copy(source_cell.alignment)\n",
        "\n",
        "                # Set row height for larger font\n",
        "                sheet_814.row_dimensions[start_row].height = 90\n",
        "\n",
        "            start_time = row['start'].strftime('%H:%M') if pd.notna(row['start']) else ''\n",
        "            end_time = row['end'].strftime('%H:%M') if pd.notna(row['end']) else ''\n",
        "            duration = ''\n",
        "            if pd.notna(row['start']) and pd.notna(row['end']):\n",
        "                duration = (row['end'] - row['start']).total_seconds() / 60\n",
        "                duration = round(duration, 0)\n",
        "\n",
        "            location_val = str(row.get('location', '')).strip()\n",
        "            location = dict_locations.get(location_val, location_val)\n",
        "            stop_group_val = str(row.get('stopGroup', '')).strip()\n",
        "            stop_group = dict_stop_type.get(stop_group_val, stop_group_val)\n",
        "            stop_val = str(row.get('stop', '')).strip()\n",
        "            stop = dict_stop_reasons.get(stop_val, stop_val)\n",
        "            comment = str(row.get('comment', '')).strip()\n",
        "            combined = f\"{stop} ({comment})\".strip()\n",
        "\n",
        "            set_left_font48(sheet_814[f\"E{start_row}\"], start_time)\n",
        "            set_left_font48(sheet_814[f\"F{start_row}\"], end_time)\n",
        "            set_left_font48(sheet_814[f\"G{start_row}\"], duration)\n",
        "            set_left_font48(sheet_814[f\"H{start_row}\"], location)\n",
        "            set_left_font48(sheet_814[f\"K{start_row}\"], stop_group)\n",
        "            set_left_font48(sheet_814[f\"N{start_row}\"], combined)\n",
        "\n",
        "            start_row += 1\n",
        "\n",
        "    # 🔁 Speed losses API\n",
        "    speed_url = f\"https://api.evocon.com/api/reports/speedlosses_json?stationId={station_id}&startTime=2025-03-01&endTime=2050-01-01\"\n",
        "    response = requests.get(speed_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Speed losses API error: {response.status_code}\")\n",
        "\n",
        "    speed_df = pd.DataFrame(response.json())\n",
        "    speed_df['start'] = pd.to_datetime(speed_df['start'], errors='coerce')\n",
        "    speed_df['shiftName'] = speed_df['shiftName'].astype(str).str.strip()\n",
        "    speed_df['performanceLossNotes'] = speed_df['performanceLossNotes'].astype(str).fillna('').str.strip()\n",
        "    speed_df['stopMinutes'] = pd.to_numeric(speed_df['stopMinutes'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Filter by selected date and shift\n",
        "    if shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        speed_filtered = speed_df[\n",
        "            ((speed_df['start'].dt.date == selected_date) & (speed_df['start'].dt.time > time(21, 30))) |\n",
        "            ((speed_df['start'].dt.date == next_day) & (speed_df['start'].dt.time <= time(5, 30)))\n",
        "        ]\n",
        "    else:\n",
        "        speed_filtered = speed_df[\n",
        "            (speed_df['start'].dt.date == selected_date) &\n",
        "            (speed_df['shiftName'] == shift_label)\n",
        "        ]\n",
        "\n",
        "    # Group by comment (performanceLossNotes)\n",
        "    grouped_speed = speed_filtered.groupby('performanceLossNotes')\n",
        "    for comment_val, group in grouped_speed:\n",
        "      total_duration = round(group['stopMinutes'].sum(), 2)\n",
        "      comment = dict_speed_loss_reasons.get(comment_val, comment_val)\n",
        "\n",
        "\n",
        "      if start_row >= 49:\n",
        "        sheet_814.insert_rows(start_row)\n",
        "        for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "            if merged_range.min_row == start_row:\n",
        "                try:\n",
        "                    sheet_814.unmerge_cells(str(merged_range))\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        # Copy style from row 49 to the new row, including row height\n",
        "        copy_row_format(sheet_814, 40, start_row)\n",
        "\n",
        "\n",
        "    # Write values to sheet\n",
        "      set_left(sheet_814[f\"E{start_row}\"], \"gubitak brzine\")\n",
        "      set_left(sheet_814[f\"F{start_row}\"], \"gubitak brzine\")\n",
        "      set_left(sheet_814[f\"G{start_row}\"], total_duration)\n",
        "      set_left(sheet_814[f\"K{start_row}\"], dict_speed_loss_group.get(\"Speed Loss\", \"Speed Loss\")) # Always \"Speed Loss\" group\n",
        "      set_left(sheet_814[f\"N{start_row}\"], comment)\n",
        "\n",
        "      start_row += 1\n",
        "\n",
        "\n",
        "\n",
        "    # 🚀 STEP: Fill product info (product name, SKU, start/end time of runs) into Sheet1\n",
        "\n",
        "\n",
        "    # 🧩 1. Get shift time boundaries\n",
        "    shift_start_time, shift_end_time = get_shift_bounds(shift, selected_date)\n",
        "\n",
        "    # 👀 2. Get first product info from the first stop\n",
        "    first_stop_row = filtered_losses.iloc[0]\n",
        "    first_product_name = str(first_stop_row.get(\"productName\", \"\")).strip()\n",
        "    first_product_sku = str(first_stop_row.get(\"productSku\", \"\")).strip()\n",
        "\n",
        "    # 🖋️ Fill F10 (SKU), F11 (product), and J10 (shift start time) initially\n",
        "    sheet_814[\"F10\"].value = first_product_sku\n",
        "    sheet_814[\"F11\"].value = first_product_name\n",
        "    sheet_814[\"J10\"].value = shift_start_time.strftime(\"%H:%M\")\n",
        "\n",
        "    # ⏳ 3. Fetch production runs and filter those within this shift\n",
        "    prod_url = f\"https://api.evocon.com/api/reports/production_runs_json?startTime=2025-03-01&endTime=2050-01-01&stationId={station_id}\"\n",
        "    prod_df = pd.DataFrame(requests.get(prod_url, headers=headers).json())\n",
        "\n",
        "    # Normalize times and dates\n",
        "    prod_df['startDate'] = pd.to_datetime(prod_df['startDate'], errors='coerce').dt.date\n",
        "    prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
        "    prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n",
        "\n",
        "    # For 3rd shift, we must span dates\n",
        "    if shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        shift_filter = (\n",
        "            ((prod_df['startDate'] == selected_date) & (prod_df['startTime'].dt.time >= shift_start_time)) |\n",
        "            ((prod_df['startDate'] == next_day) & (prod_df['startTime'].dt.time <= shift_end_time))\n",
        "        )\n",
        "    else:\n",
        "        shift_filter = (\n",
        "            (prod_df['startDate'] == selected_date) &\n",
        "            (prod_df['startTime'].dt.time >= shift_start_time) &\n",
        "            (prod_df['startTime'].dt.time <= shift_end_time)\n",
        "        )\n",
        "\n",
        "    prod_filtered = prod_df[shift_filter].sort_values(by=\"startTime\").reset_index(drop=True)\n",
        "\n",
        "    # 🧠 4. Build initial production run list from stop data\n",
        "    production_runs = [{\n",
        "        \"sku\": first_product_sku,\n",
        "        \"product\": first_product_name,\n",
        "        \"start_time\": shift_start_time.strftime(\"%H:%M\"),\n",
        "        \"end_time\": None  # to be filled later\n",
        "    }]\n",
        "\n",
        "    # 🔍 5. Detect \"Changeover and CIP\" stops to determine end/start transitions\n",
        "    changeover_rows = filtered_losses[filtered_losses[\"stopGroup\"] == \"Changeover and CIP\"].copy()\n",
        "    changeover_rows = changeover_rows.sort_values(by=\"start\").reset_index(drop=True)\n",
        "\n",
        "    # Assign end time of first run\n",
        "    if not changeover_rows.empty and pd.notna(changeover_rows.iloc[0][\"start\"]):\n",
        "        production_runs[0][\"end_time\"] = changeover_rows.iloc[0][\"start\"].strftime(\"%H:%M\")\n",
        "    else:\n",
        "        production_runs[0][\"end_time\"] = shift_end_time.strftime(\"%H:%M\")\n",
        "\n",
        "\n",
        "    # 🔁 6. Append production runs from prod_filtered (with proper changeover sequencing)\n",
        "    changeover_rows = changeover_rows.reset_index(drop=True)  # Safe for iloc\n",
        "    all_stops = filtered_losses.reset_index(drop=True)        # Ensure same for full stops list\n",
        "\n",
        "    changeover_idx = 0\n",
        "    last_valid_end = None\n",
        "\n",
        "    for i, row in prod_filtered.iterrows():\n",
        "        sku = str(row.get(\"sku\", \"\")).strip()\n",
        "        product = str(row.get(\"product\", \"\")).strip()\n",
        "\n",
        "        # 🧩 Find the last \"Changeover and CIP\" in a consecutive sequence\n",
        "        cip_end_time = None\n",
        "        while changeover_idx < len(changeover_rows):\n",
        "            current_cip = changeover_rows.iloc[changeover_idx]\n",
        "            changeover_idx += 1\n",
        "\n",
        "            # If last or next stop is not \"Changeover and CIP\", this is the last in sequence\n",
        "            if changeover_idx == len(changeover_rows):\n",
        "                cip_end_time = current_cip.get(\"end\")\n",
        "                break\n",
        "\n",
        "            next_cip = changeover_rows.iloc[changeover_idx]\n",
        "            curr_idx_in_all = all_stops[all_stops['start'] == current_cip['start']].index[0]\n",
        "\n",
        "            if curr_idx_in_all + 1 < len(all_stops):\n",
        "                next_stop_group = all_stops.iloc[curr_idx_in_all + 1].get(\"stopGroup\", \"\")\n",
        "                if next_stop_group != \"Changeover and CIP\":\n",
        "                    cip_end_time = current_cip.get(\"end\")\n",
        "                    break\n",
        "\n",
        "        # 🕒 Start time = end of last CIP, or start of shift\n",
        "        if pd.notna(cip_end_time):\n",
        "            start_time = cip_end_time\n",
        "        else:\n",
        "            start_time = row.get(\"startTime\") or shift_start_time\n",
        "\n",
        "        start_str = start_time.strftime(\"%H:%M\") if pd.notna(start_time) else \"\"\n",
        "\n",
        "        # 🕛 End time = next CIP start, or shift end\n",
        "        if changeover_idx < len(changeover_rows):\n",
        "            next_cip_start = changeover_rows.iloc[changeover_idx][\"start\"]\n",
        "            end_str = next_cip_start.strftime(\"%H:%M\") if pd.notna(next_cip_start) else shift_end_time.strftime(\"%H:%M\")\n",
        "        else:\n",
        "            end_str = shift_end_time.strftime(\"%H:%M\")\n",
        "\n",
        "        production_runs.append({\n",
        "            \"sku\": sku,\n",
        "            \"product\": product,\n",
        "            \"start_time\": start_str,\n",
        "            \"end_time\": end_str\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #version of insert rows and writing with font and size formatting and with row height adjustment\n",
        "\n",
        "\n",
        "    # 🔠 Font config\n",
        "    prod_font = Font(name=\"Arial\", size=48)\n",
        "    bold_font = Font(bold=True)\n",
        "\n",
        "    # 🧱 Insert new rows (if more than one product)\n",
        "    insert_point = 10\n",
        "    extra_runs = len(production_runs) - 1\n",
        "\n",
        "    if extra_runs > 0:\n",
        "        for _ in range(extra_runs * 2):\n",
        "            sheet_814.insert_rows(insert_point)\n",
        "\n",
        "    # 🧾 content to be written in new cells inserted\n",
        "    title_map = {\n",
        "        \"E\": \"Šifra:\",\n",
        "        \"E_lower\": \"Naziv proizvoda:\",\n",
        "        \"H\": \"Proizvodnja od (prva boca kontinuirano na punjaču):\",\n",
        "        \"H_lower\": \"Proizvodnja do (zadnja boca na punjaču):\",\n",
        "    }\n",
        "\n",
        "    # 🧬 Apply values and formatting row by row\n",
        "    for i, run in enumerate(production_runs):\n",
        "        row_top = 10 + (i * 2)\n",
        "        row_bottom = row_top + 1\n",
        "\n",
        "        # Copy titles to column E and H\n",
        "        sheet_814[f\"E{row_top}\"].value = title_map[\"E\"]\n",
        "        sheet_814[f\"E{row_bottom}\"].value = title_map[\"E_lower\"]\n",
        "        sheet_814[f\"H{row_top}\"].value = title_map[\"H\"]\n",
        "        sheet_814[f\"H{row_bottom}\"].value = title_map[\"H_lower\"]\n",
        "\n",
        "        for col in ['E', 'F', 'H', 'J']:\n",
        "            for r in [row_top, row_bottom]:\n",
        "                cell = sheet_814[f\"{col}{r}\"]\n",
        "                cell.font = prod_font\n",
        "                cell.alignment = Alignment(horizontal='left', wrap_text=True)\n",
        "\n",
        "        # Fill values for product info\n",
        "        set_left(sheet_814[f\"F{row_top}\"], run[\"sku\"])\n",
        "        set_left(sheet_814[f\"F{row_bottom}\"], run[\"product\"])\n",
        "        set_left(sheet_814[f\"J{row_top}\"], run[\"start_time\"])\n",
        "        set_left(sheet_814[f\"J{row_bottom}\"], run[\"end_time\"])\n",
        "        # ovdje je u og kodu bilo height = 159\n",
        "\n",
        "    # Adjust row height for readability\n",
        "        sheet_814.row_dimensions[row_top].height = 159\n",
        "        sheet_814.row_dimensions[row_bottom].height = 159\n",
        "    \"\"\"\n",
        "        # Dynamically adjust row height based on the maximum number of lines in the cells\n",
        "        def get_max_lines(row_num, columns):\n",
        "            max_lines = 1\n",
        "            for col in columns:\n",
        "                cell_value = str(sheet_814[f\"{col}{row_num}\"].value or \"\")\n",
        "                lines = cell_value.count('\\n') + 1\n",
        "                max_lines = max(max_lines, lines)\n",
        "            return max_lines\n",
        "\n",
        "        font_size = prod_font.size if hasattr(prod_font, 'size') else 12\n",
        "        line_height = font_size * 1.2  # Approximate line height\n",
        "        for r in [row_top, row_bottom]:\n",
        "            max_lines = get_max_lines(r, ['E', 'F', 'H', 'J'])\n",
        "            sheet_814.row_dimensions[r].height = max(30, int(line_height * max_lines))\n",
        "    \"\"\"\n",
        "    # Automatically adjust row height to fit all text\n",
        "    #for r in [row_top, row_bottom]:\n",
        "     #   max_height = 15  # minimum height\n",
        "      #  for col in ['E', 'F', 'H', 'J']:\n",
        "       #     cell = sheet_814[f\"{col}{r}\"]\n",
        "        #    if cell.value:\n",
        "                # Estimate height: 1.2x font size per line, count lines by splitting on '\\n'\n",
        "         #       lines = str(cell.value).split('\\n')\n",
        "          #      num_lines = max(len(lines), 1)\n",
        "           ##    max_height = max(max_height, height)\n",
        "        #sheet_814.row_dimensions[r].height = max_height\n",
        "\n",
        "\n",
        "    #footer = 6 #doljnja sastavnica\n",
        "\n",
        "    def make_header(from_sheet, header_rows):\n",
        "    #from_sheet se samo referira na sheet excela\n",
        "    # postavljanje headera kao rows\n",
        "        from_sheet.print_title_rows = f'1:{header_rows}'\n",
        "\n",
        "    header = 30 #gornja sastavnica\n",
        "\n",
        "# mozda bi radilo da u taj length dodam AND raspon footera al idk bi li slomilo kod\n",
        "\n",
        "    def make_footer(from_sheet, footer_rows):\n",
        "        max_row = from_sheet.max_row\n",
        "        footer_texts = []\n",
        "        for row in range(max_row - footer_rows, max_row + 1):\n",
        "            row_values = []\n",
        "            for col in range(1, from_sheet.max_column + 1):\n",
        "                cell = from_sheet.cell(row=row, column=col)\n",
        "                if cell.value:\n",
        "                    row_values.append(str(cell.value))\n",
        "            if row_values:\n",
        "                footer_texts.append(\" | \".join(row_values))\n",
        "        # pretvori sve u string jer excel supporta samo string footers iz nekog razloga\n",
        "        footer_str = \"\\n\".join(footer_texts)\n",
        "        # Set footer font explicitly to Arial, size 48\n",
        "        font_str = '&\"Arial\"&48'\n",
        "        formatted_footer = f'{font_str}{footer_str}'\n",
        "        from_sheet.oddFooter.center.text = formatted_footer\n",
        "\n",
        "    make_header(sheet_814, header)\n",
        "    # make_footer(sheet_814, footer)\n",
        "\n",
        "    # 💾 Step 11: Save and download\n",
        "    from openpyxl.utils import get_column_letter\n",
        "\n",
        "# namjesti print area na cijeli range\n",
        "# jer ako netko ne klikne \"ignore print range\" pri printanju\n",
        "# nece iskopirati sve redove\n",
        "    max_row = sheet_814.max_row\n",
        "    max_col = sheet_814.max_column\n",
        "    print_area = f\"A1:{get_column_letter(max_col)}{max_row}\"\n",
        "    sheet_814.print_area = print_area\n",
        "\n",
        "\n",
        "    #output_file = output_dir / f\"updated_shift_report_{line_input}_{date_str}.xlsx\"\n",
        "    #wb.save(output_file)\n",
        "\n",
        "    # Define the output directory\n",
        "    #output_dir = \"excel_reports\"\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    #if not os.path.exists(output_dir):\n",
        "    #  os.makedirs(output_dir)\n",
        "\n",
        "    # Save the Excel file\n",
        "    #output_file = f\"{output_dir}/updated_shift_report_{line_input}_{date_str}.xlsx\"\n",
        "    #wb.save(output_file)\n",
        "\n",
        "    output_file = f\"{line_input}_{shift}_{date_str}.xlsx\"\n",
        "    wb.save(output_file)\n",
        "    # Create the folder (if it doesn't exist)\n",
        "    os.makedirs(\"excel_reports\", exist_ok=True)\n",
        "\n",
        "    # Move the file to the output directory\n",
        "    os.replace(output_file, os.path.join(\"excel_reports\", output_file))\n",
        "\n",
        "    #output_file = \"updated_shift_report.xlsx\"\n",
        "    #wb.save(output_file)\n",
        "    #files.download(output_file)\n",
        "    #wb.save(full_path)\n",
        "    #wb.save(output_file)\n",
        "    #files.download(output_file)\n",
        "    \"\"\"\n",
        "        # Git commit and push\n",
        "    try:\n",
        "      print(\"---GIT ADD---\")\n",
        "      subprocess.run([\"git\", \"add\", str(full_path)], check=True, capture_output=True, text=True)\n",
        "      print(\"---GIT COMMIT---\")\n",
        "      subprocess.run([\"git\", \"commit\", \"-m\", f\"Add Excel file {filename}\"], check=True, capture_output=True, text=True)\n",
        "      print(\"---GIT PUSH---\")\n",
        "      subprocess.run([\"git\", \"push\"], check=True, capture_output=True, text=True)\n",
        "      print(\"✅ Excel file committed and pushed to GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "      print(f\"❌ Git operation failed: {e}\")\n",
        "      print(f\"---STDOUT---:\\n{e.stdout}\")\n",
        "      print(f\"---STDERR---:\\n{e.stderr}\")\n",
        "    \"\"\""
      ]
    }
  ]
}