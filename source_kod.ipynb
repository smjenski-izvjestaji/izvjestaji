{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUr2zOX7BjPM7adZjN87ad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smjenski-izvjestaji/izvjestaji/blob/main/source_kod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "FJsSHJ32aKUN",
        "outputId": "73ba02f3-a89e-48fd-a1df-932841b77548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Processing line: RGB (Station ID: 1)\n",
            "✅ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3462720292.py:557: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-3462720292.py:558: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b1ef77b1-9737-44bf-a30a-d3e215e27c7f\", \"updated_shift_report_RGB_20250819.xlsx\", 11208)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: PET1 (Station ID: 2)\n",
            "✅ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3462720292.py:557: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-3462720292.py:558: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6d603eea-2539-4993-8030-0c6c230dccec\", \"updated_shift_report_PET1_20250819.xlsx\", 10647)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: PET2 (Station ID: 3)\n",
            "✅ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3462720292.py:557: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-3462720292.py:558: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_851522c4-e760-4f04-a657-3081aa43e85e\", \"updated_shift_report_PET2_20250819.xlsx\", 10700)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n        # Git commit and push\\n    try:\\n      print(\"---GIT ADD---\")\\n      subprocess.run([\"git\", \"add\", str(full_path)], check=True, capture_output=True, text=True)\\n      print(\"---GIT COMMIT---\")\\n      subprocess.run([\"git\", \"commit\", \"-m\", f\"Add Excel file {filename}\"], check=True, capture_output=True, text=True)\\n      print(\"---GIT PUSH---\")\\n      subprocess.run([\"git\", \"push\"], check=True, capture_output=True, text=True)\\n      print(\"✅ Excel file committed and pushed to GitHub.\")\\n    except subprocess.CalledProcessError as e:\\n      print(f\"❌ Git operation failed: {e}\")\\n      print(f\"---STDOUT---:\\n{e.stdout}\")\\n      print(f\"---STDERR---:\\n{e.stderr}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install pandas openpyxl requests\n",
        "\n",
        "# Step 2: Upload the Excel template\n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import time as systime\n",
        "import ast\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.cell.cell import MergedCell\n",
        "from openpyxl.utils import get_column_letter\n",
        "from datetime import datetime, time\n",
        "from openpyxl.styles import Alignment, Font, Border\n",
        "import copy\n",
        "\n",
        "import requests\n",
        "\n",
        "from datetime import datetime, time\n",
        "from openpyxl import Workbook\n",
        "\n",
        "from datetime import datetime, time, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "\n",
        "\n",
        "import openpyxl\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "dict_stop_reasons={\"Electrical Failure\":\"Električni kvar\", \"Bottle Necks\":\"Usko grlo\", \"COP\":\"COP\", \"End of production\":\"Završetak proizvodnje\", \"Not recorded\":\"Nije zabilježen\", \"Preventive Maintenance\":\"Preventivno održavanje\", \"Changeover\":\"Changeover\", \"Other failure\":\"Drugi kvar\", \"Adjustments on equipment\":\"Prilagodbe na opremi\", \"CIP (5ST)\":\"CIP (5ST)\", \"Mechanical failure\":\"Mehanički kvar\",\"Meeting\":\"Sastanak\", \"Start of production\":\"Početak proizvodnje\", \"CIP (3V)\":\"CIP (3V)\", \"Cleaning on line\":\"Čišćenje na liniji\", \"CIP (3L)\":\"CIP (3L)\", \"Insufficient Electrical en.supply\":\"Nedovoljna opskrba elektr. energije\",\"CIP (OV)\":\"CIP (OV)\",\"Insufficient Water supply\":\"Nedovoljna opskrba vode\",\"Lack of personnel\":\"Nedostatak operatera\",\"Changeover + CIP (3V)\":\"Changeover + CIP (3V)\",\"Not skilled personnel\":\"Nekvalificirani operateri\",\"Changeover + CIP (3L)\":\"Changeover + CIP (3L)\",\"Other\":\"Drugo\",\"Changeover + CIP (OV)\":\"Changeover + CIP (OV)\",\"QA tests on the line\":\"QA testovi na liniji\",\"Test bottle\":\"Probna boca\",\"Falling bottles on conveyors\":\"Padaju boce na transporterima\",\"P1 (CHO 12/24)\":\"P1 (CHO 12/24)\",\"Stopped production from QA\":\"Zaustavljena proizvodnja iz QA\",\"P2 (CHO filler)\":\"P2 (CHO filler)\",\"Insufficient CO2 supply\":\"Nedovoljna opskrba CO2\",\"P3 (CHO+OV+MIX)\":\"P3 (CHO+OV+MIX)\",\"Waiting for the Syrup\":\"Čekanje sirupa\",\"P4 (CHO+3°+MIX)\":\"P4 (CHO+3°+MIX)\",\"Insufficient Cold water supply\":\"Nedovoljna opskrba hladne vode\",\"P5 (CHO+5°+MIX)\":\"P5 (CHO+5°+MIX)\",\"Poor CHO performance\":\"Loš performans changeovera\",\"P6 (OV+MIX)\":\"P6 (OV+MIX)\",\"Poor Bottles Quality\":\"Loša kvaliteta boca\",\"P7 (3°+MIX)\":\"P7 (3°+MIX)\",\"Poor Crates Quality\":\"Loša kvaliteta sanduka\",\"P8 (5°+MIX)\":\"P8 (5°+MIX)\",\"Poor Labels Quality\":\"Loša kvaliteta etiketa\",\"P9 (DZ+OV+MIX)\":\"P9 (DZ+OV+MIX)\",\"Poor Pallets (Wood) Quality\":\"Loša kvaliteta drvenih paleta\",\"P10 (DZ+3°+MIX)\":\"P10 (DZ+3°+MIX)\",\"Poor Cap Quality\":\"Loša kvaliteta zatvarača\",\"P11 (DZ+5°+MIX)\":\"P11 (DZ+5°+MIX)\",\"Poor support from Storage\":\"Loša podrška iz skladišta\",\"Poor lubrication of conveyors\":\"Slabo podmazivanje transportera\",\"Poor thermofoil quality\":\"Loša kvaliteta termofolije\",\"Poor Preform Quality\":\"Loša kvaliteta predforme\",\"Replenishment of raw materials\":\"Nadopuna repromaterijala\",\"Shift break\":\"Smjenska pauza\",\"Mixed crates\":\"Miješani sanduci\",\"Mixed bottles\":\"Miješane boce\",\"Dirty crates\":\"Prljavi sanduci\",\"Dirty bottles\":\"Prljave boce\",\"test\":\"test\",\"Uncommented\":\"Bez komentara\"}\n",
        "\n",
        "dict_stop_type={\"EPL\":\"EPL\",\"OPL\":\"OPL\",\"Non scheduled time\":\"Neplanirano vrijeme\",\"Changeover and CIP\":\"Changeover i CIP\",\"Maintenance and Set up\":\"Održavanje i setup\",\"Minor Stoppages\":\"Manji zastoji\",\"Preventive Maintenance\":\"Preventivno održavanje\",\"Speed Losses\":\"Gubici brzine\",\"Uncommented\":\"Bez komentara\"}\n",
        "\n",
        "dict_speed_loss_reasons={\"Speed loss\":\"Gubitak brzine\"}\n",
        "\n",
        "dict_speed_loss_group={\"Speed Loss\":\"Gubitak brzine\"}\n",
        "\n",
        "dict_locations={\"Infeed of caser\":\"Ulaz u upakivač\",\"Crates conveyors from caser to palletizer\":\"Transporteri sanduka od upakivača do paletizera\",\"Bottles conveyor from uncaser to washer machine\":\"Transporter boca od ispakivača do perilice\",\"Bottle washer\":\"Perilica boca\",\"Bottles conveyor from washer to filler\":\"Transporter boca od perilice do punjača\",\"Linatronic\":\"Linatronic\",\"Mixer\":\"Mixer\",\"Bottles conveyor from filler to labelling machine\":\"Transporter boca od punjača do etiketirke\",\"Bottles conveyor from labelling machine to caser\":\"Transporter boca od etiketirke do upakivača\",\"Caser\":\"Upakivač\",\"Crates conveyors from depalletizer to uncaser\":\"Transporteri sanduka od depaletizera do ispakivača\",\"Uncaser\":\"Ispakivač\",\"Infeed of bottle washer\":\"Ulaz u perilicu boca\",\"Outfeed from washer machine\":\"Izlaz iz perilice boca\",\"Inliner in white line\":\"Inliner u bijeloj liniji\",\"Cap conveyors\":\"Transporteri zatvarača\",\"Checkmat after filler\":\"Checkmat nakon punjača\",\"Depalletizer\":\"Depaletizer\",\"Filler\":\"Punjač\",\"Palletizer\":\"Paletizer\",\"Labeller\":\"Etiketirka\",\"Crates conveyor from uncaser to washer\":\"Transporter sanduka od ispakivača do perilice sanduka\",\"Crates conveyor from washer to caser\":\"Transporter sanduka od perilice sanduka do upakivača\",\"Crates washer\":\"Perilica sanduka\",\"Packer (Zambelli)\":\"Pakiralica (Zambelli)\",\"Blower\":\"Puhalica\",\"Packer (KHS)\":\"Pakiralica (KHS)\",\"Handle machine\":\"Ručkomat\",\"Air conveyor\":\"Zračni transporter\",\"Conveyor from labeller to packer (KHS)\":\"Transporter od etiketirke do pakiralice (KHS)\",\"Conveyor from KHS to handle machine\":\"Transporter od KHS do ručkomata\",\"Conveyor from handle machine to Zambelli\":\"Transporter od ručkomata do Zambelli-a\",\"Conveyor from packer (Zambelli) to palletizer\":\"Transporter od pakiralice (Zambelli) do paletizera\",\"Conveyor from labeller to packer (Zambelli)\":\"Transporter od etiketirke do pakiralice (Zambelli)\",\"Entire line\":\"Cijela linija\",\"Water treatment\":\"Obrada vode\",\"Syrup room\":\"Sirupana\",\"Compressor room\":\"Kompresorska stanica\",\"Cooler\":\"Hladnjak\",\"Rinser\":\"Ispiračica\",\"Capper\":\"Zatvaračica\",\"Date printer\":\"Datumar\",\"Case sticker applicator\":\"Aplikator naljepnica na paket\",\"Pallet sticker applicator\":\"Aplikator naljepnica na paletu\",\"Foam maker\":\"Pjenomat\"}\n",
        "\n",
        "\n",
        "# --- Helper: Copy all formatting from a template row to a target row ---\n",
        "from openpyxl.styles import Border\n",
        "def copy_row_format(sheet, src_row, tgt_row):\n",
        "    # Copy cell styles\n",
        "    for col in range(1, sheet.max_column + 1):\n",
        "        src_cell = sheet.cell(row=src_row, column=col)\n",
        "        tgt_cell = sheet.cell(row=tgt_row, column=col)\n",
        "        if src_cell.has_style:\n",
        "            tgt_cell.font = copy.copy(src_cell.font)\n",
        "            tgt_cell.fill = copy.copy(src_cell.fill)\n",
        "            tgt_cell.number_format = copy.copy(src_cell.number_format)\n",
        "            tgt_cell.protection = copy.copy(src_cell.protection)\n",
        "            tgt_cell.alignment = copy.copy(src_cell.alignment)\n",
        "            # Copy border (must create a new Border object)\n",
        "            src_border = src_cell.border\n",
        "            tgt_cell.border = Border(\n",
        "                left=copy.copy(src_border.left),\n",
        "                right=copy.copy(src_border.right),\n",
        "                top=copy.copy(src_border.top),\n",
        "                bottom=copy.copy(src_border.bottom),\n",
        "                diagonal=copy.copy(src_border.diagonal),\n",
        "                diagonal_direction=src_border.diagonal_direction,\n",
        "                outline=src_border.outline,\n",
        "                vertical=copy.copy(src_border.vertical),\n",
        "                horizontal=copy.copy(src_border.horizontal)\n",
        "            )\n",
        "    # Copy row height\n",
        "    if src_row in sheet.row_dimensions:\n",
        "        sheet.row_dimensions[tgt_row].height = sheet.row_dimensions[src_row].height\n",
        "\n",
        "    # Copy merged cells (only those that start in src_row)\n",
        "    for merged_range in list(sheet.merged_cells.ranges):\n",
        "        if merged_range.min_row == src_row and merged_range.max_row == src_row:\n",
        "            # Only copy single-row merges\n",
        "            new_range = merged_range.coord.replace(str(src_row), str(tgt_row))\n",
        "            # Avoid duplicate merges\n",
        "            if new_range not in [str(rng) for rng in sheet.merged_cells.ranges]:\n",
        "                sheet.merge_cells(new_range)\n",
        "\n",
        "# Station mapping\n",
        "station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "\n",
        "# Replace with your actual timezone (e.g., 'Europe/Belgrade', 'Europe/Zagreb')\n",
        "local_timezone = ZoneInfo('Europe/Zagreb')\n",
        "\n",
        "# Get local time\n",
        "now = datetime.utcnow()\n",
        "now_local = now.replace(tzinfo=ZoneInfo('UTC')).astimezone(local_timezone)\n",
        "\n",
        "current_time = now_local.time()\n",
        "\n",
        "# Determine shift and date based on local time\n",
        "# vrijeme je u UTC+0 by default a ne CEST (UTC+2) PAZI!!!\n",
        "# zato smo uveli now_local\n",
        "if time(5, 30) <= current_time < time(13, 30):\n",
        "    shift = 1\n",
        "    selected_date = now_local.date()\n",
        "elif time(13, 30) <= current_time < time(21, 30):\n",
        "    shift = 2\n",
        "    selected_date = now_local.date()\n",
        "else:\n",
        "    shift = 3\n",
        "    selected_date = (now_local - timedelta(days=1)).date()  # Use previous day\n",
        "\n",
        "# Format date for filename\n",
        "date_str = selected_date.strftime('%d%m%Y')\n",
        "\n",
        "output_dir = Path(\"excel_reports\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Iterate over each production line\n",
        "for line_input, station_id in station_map.items():\n",
        "    print(f\"Processing line: {line_input} (Station ID: {station_id})\")\n",
        "\n",
        "\n",
        "    # GitHub raw URL to the Excel template\n",
        "    template_url = \"https://raw.githubusercontent.com/smjenski-izvjestaji/izvjestaji/main/Shift_report_template.xlsx\"\n",
        "    filename = \"template.xlsx\"\n",
        "\n",
        "    # 📥 Download the file\n",
        "    response = requests.get(template_url)\n",
        "    if response.status_code == 200:\n",
        "      with open(filename, 'wb') as f:\n",
        "          f.write(response.content)\n",
        "      print(f\"✅ Template downloaded: {filename}\")\n",
        "    else:\n",
        "      raise Exception(f\"❌ Failed to download file: {response.status_code}\")\n",
        "\n",
        "    # 🧠 Step 3: Load workbook and sheet\n",
        "    #filename = next(iter(uploaded))\n",
        "    wb = load_workbook(filename)\n",
        "    ws = wb.active\n",
        "    #systime.sleep(10)  # Optional delay\n",
        "    sheet_814 = wb['Sheet1']\n",
        "\n",
        "\n",
        "    # 🔐 Step 4: Prepare API credentials\n",
        "    api_key = \"EVO99B7A17C6CF2410\"         # 🔁 Replace with your API key\n",
        "    secret_key = \"v2xLnrFQD18WYtyUEykK\"   # 🔁 Replace with your secret key\n",
        "\n",
        "    credentials = f\"{api_key}:{secret_key}\"\n",
        "    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n",
        "\n",
        "    headers = {\n",
        "    \"Authorization\": f\"Basic {encoded_credentials}\"\n",
        "    }\n",
        "\n",
        "    # Station map\n",
        "    station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "    shift_label = {1: \"1st\", 2: \"2nd\", 3: \"3rd\"}[shift]\n",
        "\n",
        "\n",
        "    #  Define shift time windows for start and end\n",
        "    def get_shift_bounds(shift, date):\n",
        "        if shift == 1:\n",
        "            return time(5, 30), time(13, 30)\n",
        "        elif shift == 2:\n",
        "            return time(13, 30), time(21, 30)\n",
        "        elif shift == 3:\n",
        "            # Third shift spans 2 dates\n",
        "            return time(21, 30), time(5, 30)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid shift number\")\n",
        "\n",
        "    output_file = f\"updated_shift_report_{line_input}_{date_str}.xlsx\"\n",
        "    full_path = output_dir / output_file\n",
        "#\n",
        "    # 🧾 Write date and shift to cells D9 and F9\n",
        "    def set_left(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='left')\n",
        "\n",
        "    def set_left_font48(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='left')\n",
        "        cell.font = Font(name=\"Arial\", size=48)\n",
        "\n",
        "    def set_center(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "    def set_right(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='right')\n",
        "\n",
        "    set_left(sheet_814[\"D9\"], selected_date.strftime(\"%d.%m.%Y\"))\n",
        "    set_left(sheet_814[\"F9\"], str(shift))\n",
        "\n",
        "    # write other info in header (punjac, linija, ...)\n",
        "\n",
        "    if line_input == \"PET1\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjač O+H\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE PET BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 866 (PET1)\")\n",
        "\n",
        "    elif line_input == \"PET2\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjač Krones\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE PET BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 814 (PET2)\")\n",
        "\n",
        "    elif line_input == \"RGB\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjač Krones\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE RGB BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 865 (RGB)\")\n",
        "\n",
        "\n",
        "\n",
        "    # 📡 Step 5: Fetch checklist data from API\n",
        "    checklist_url = f\"https://api.evocon.com/api/reports/checklists_json?startTime=2025-03-01&endTime=2050-01-01&stationId={station_id}\"\n",
        "    response = requests.get(checklist_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Checklist API error: {response.status_code}\")\n",
        "\n",
        "    checklist_df = pd.DataFrame(response.json())\n",
        "\n",
        "    # 🧾 Step 6: Normalize checklist data\n",
        "    def clean_time(t):\n",
        "        if pd.isna(t): return None\n",
        "        t = str(t).replace(\":\", \"\").strip()\n",
        "        return t.zfill(4)[-4:]\n",
        "\n",
        "    # Handle stringified lists in 'itemresult'\n",
        "    def extract_single_value(x):\n",
        "        if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(x)\n",
        "                if isinstance(parsed, list) and len(parsed) == 1:\n",
        "                    return str(parsed[0])\n",
        "            except:\n",
        "                return x\n",
        "        return str(x).strip()\n",
        "\n",
        "    checklist_df['clean_duetime'] = checklist_df['duetime'].apply(clean_time)\n",
        "    checklist_df['duetime_dt'] = pd.to_datetime(checklist_df['duetime'], format='%H:%M', errors='coerce').dt.time\n",
        "    checklist_df['date'] = pd.to_datetime(checklist_df['date'], errors='coerce').dt.date\n",
        "    checklist_df['itemname'] = checklist_df['itemname'].astype(str).str.strip().str.lower()\n",
        "    checklist_df['result'] = checklist_df['result'].astype(str).str.strip().str.lower()\n",
        "    checklist_df['itemresult'] = checklist_df['itemresult'].apply(extract_single_value)\n",
        "\n",
        "    # Apply shift filter\n",
        "    if shift == 1:\n",
        "        time_filter = (checklist_df['date'] == selected_date) & \\\n",
        "                      (checklist_df['duetime_dt'] >= time(6, 0)) & \\\n",
        "                      (checklist_df['duetime_dt'] <= time(14, 0))\n",
        "    elif shift == 2:\n",
        "        time_filter = (checklist_df['date'] == selected_date) & \\\n",
        "                      (checklist_df['duetime_dt'] > time(14, 0)) & \\\n",
        "                      (checklist_df['duetime_dt'] <= time(22, 0))\n",
        "    elif shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        time_filter = (\n",
        "            ((checklist_df['date'] == selected_date) & (checklist_df['duetime_dt'] > time(22, 0))) |\n",
        "            ((checklist_df['date'] == next_day) & (checklist_df['duetime_dt'] <= time(5, 46)))\n",
        "        )\n",
        "\n",
        "    checklist_df = checklist_df[time_filter]\n",
        "\n",
        "    # Map parameter names from D17–D27\n",
        "    param_row_map = {}\n",
        "    for row in range(17, 28):\n",
        "        val = sheet_814[f\"D{row}\"].value\n",
        "        if val:\n",
        "            param_row_map[val.strip().lower()] = row\n",
        "\n",
        "    # Match headers in row 16\n",
        "    header_map = {}\n",
        "    for col in sheet_814.iter_cols(min_row=16, max_row=16):\n",
        "        cell = col[0]\n",
        "        if cell.value:\n",
        "            values = [p.strip().replace(\":\", \"\") for p in str(cell.value).split('/')]\n",
        "            if len(values) == 3:\n",
        "                header_map[col[0].column_letter] = {\n",
        "                    'morning': values[0].zfill(4),\n",
        "                    'afternoon': values[1].zfill(4),\n",
        "                    'night': values[2].zfill(4)\n",
        "                }\n",
        "\n",
        "    # Write checklist values\n",
        "    for _, row in checklist_df.iterrows():\n",
        "        if row['result'] != 'successful':\n",
        "            continue\n",
        "\n",
        "        param_name = row['itemname']\n",
        "        result = row['itemresult']\n",
        "        duetime = row['duetime_dt']\n",
        "        duetime_str = row['clean_duetime']\n",
        "\n",
        "        if pd.isna(duetime) or param_name not in param_row_map:\n",
        "            continue\n",
        "\n",
        "        shift_key = None\n",
        "        if time(6, 15) <= duetime <= time(13, 45):\n",
        "            shift_key = 'morning'\n",
        "        elif time(14, 15) <= duetime <= time(21, 45):\n",
        "            shift_key = 'afternoon'\n",
        "        elif duetime >= time(22, 15) or duetime <= time(5, 45):\n",
        "            shift_key = 'night'\n",
        "\n",
        "        target_column = None\n",
        "        for col_letter, shift_times in header_map.items():\n",
        "            if shift_times.get(shift_key) == duetime_str:\n",
        "                target_column = col_letter\n",
        "                break\n",
        "\n",
        "        if not target_column:\n",
        "            continue\n",
        "\n",
        "        row_number = param_row_map[param_name]\n",
        "        cell = sheet_814[f\"{target_column}{row_number}\"]\n",
        "        if not isinstance(cell, MergedCell):\n",
        "            set_left(cell, result)\n",
        "\n",
        "    # 🌐 API 2: Stop reasons\n",
        "    losses_url = f\"https://api.evocon.com/api/reports/losses_json?stationId={station_id}&startTime=2025-03-01&endTime=2050-01-01\"\n",
        "    response = requests.get(losses_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Losses API error: {response.status_code}\")\n",
        "\n",
        "    losses_df = pd.DataFrame(response.json())\n",
        "\n",
        "    losses_df['shiftStart'] = pd.to_datetime(losses_df['shiftStart'], errors='coerce')\n",
        "    losses_df['start'] = pd.to_datetime(losses_df['start'], errors='coerce')\n",
        "    losses_df['end'] = pd.to_datetime(losses_df['end'], errors='coerce')\n",
        "\n",
        "    filtered_losses = losses_df[\n",
        "        (losses_df['shiftStart'].dt.date == selected_date) &\n",
        "        (losses_df['shiftName'] == shift_label)\n",
        "    ]\n",
        "\n",
        "# Write stops to 814 starting from row 32\n",
        "# New scrit with joined stops\n",
        "\n",
        "    start_row = 32\n",
        "    processed_join_ids = set()\n",
        "\n",
        "    for idx, row in filtered_losses.iterrows():\n",
        "        join_id = row.get('stopJoinId')\n",
        "\n",
        "        if join_id and str(join_id).strip() != \"\":\n",
        "            if join_id in processed_join_ids:\n",
        "                continue\n",
        "            processed_join_ids.add(join_id)\n",
        "\n",
        "            # ✅ get full join group from losses_df (not just filtered)\n",
        "            group = losses_df[losses_df['stopJoinId'] == join_id].copy()\n",
        "            group = group.sort_values(by='start')\n",
        "\n",
        "            total_duration = 0\n",
        "            for _, sub in group.iterrows():\n",
        "                if pd.notna(sub['start']) and pd.notna(sub['end']):\n",
        "                    total_duration += (sub['end'] - sub['start']).total_seconds() / 60\n",
        "            total_duration = round(total_duration, 0)\n",
        "\n",
        "            location = dict_locations.get(group['location'].dropna().astype(str).str.strip().iloc[0], group['location'].dropna().astype(str).str.strip().iloc[0] if not group['location'].dropna().empty else '')\n",
        "            stop_group_val = group['stopGroup'].dropna().astype(str).str.strip().iloc[0] if not group['stopGroup'].dropna().empty else ''\n",
        "            stop_group = dict_stop_type.get(stop_group_val, stop_group_val)\n",
        "            stop_val = str(group['stop'].iloc[0]).strip()\n",
        "            stop = dict_stop_reasons.get(stop_val, stop_val)\n",
        "            comment = str(group['comment'].iloc[0]).strip()\n",
        "            combined = f\"{stop} ({comment})\".strip()\n",
        "\n",
        "            if start_row >= 49:\n",
        "                sheet_814.insert_rows(start_row)\n",
        "\n",
        "                # 🧽 Unmerge any cells that were auto-merged in the inserted row\n",
        "                for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                    if merged_range.min_row == start_row:\n",
        "                        try:\n",
        "                            sheet_814.unmerge_cells(str(merged_range))\n",
        "                        except KeyError:\n",
        "                            pass # merged cell may already have been cleared\n",
        "\n",
        "                # 🎨 Copy style from row 49 to the new row\n",
        "\n",
        "\n",
        "            # --- Robustly copy all formatting from row before 49 to the new row ---\n",
        "                copy_row_format(sheet_814, 48, start_row)\n",
        "\n",
        "\n",
        "            set_left_font48(sheet_814[f\"E{start_row}\"], \"Spojeni zastoji\")\n",
        "            set_left_font48(sheet_814[f\"F{start_row}\"], \"Spojeni zastoji\")\n",
        "            set_left_font48(sheet_814[f\"G{start_row}\"], total_duration)\n",
        "            set_left_font48(sheet_814[f\"H{start_row}\"], location)\n",
        "            set_left_font48(sheet_814[f\"K{start_row}\"], stop_group)\n",
        "            set_left_font48(sheet_814[f\"N{start_row}\"], combined)\n",
        "\n",
        "            start_row += 1\n",
        "\n",
        "        else:\n",
        "            # ✅ true individual stop (no join ID or empty)\n",
        "            if start_row >= 49:\n",
        "                sheet_814.insert_rows(start_row)\n",
        "\n",
        "                # 🧽 Unmerge any cells that were auto-merged in the inserted row\n",
        "                for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                    if merged_range.min_row == start_row:\n",
        "                        try:\n",
        "                            sheet_814.unmerge_cells(str(merged_range))\n",
        "                        except KeyError:\n",
        "                            pass # merged cell may already have been cleared\n",
        "\n",
        "\n",
        "                # --- Robustly copy all formatting from row 49 to the new row ---\n",
        "                copy_row_format(sheet_814, 48, start_row)\n",
        "\n",
        "            start_time = row['start'].strftime('%H:%M') if pd.notna(row['start']) else ''\n",
        "            end_time = row['end'].strftime('%H:%M') if pd.notna(row['end']) else ''\n",
        "            duration = ''\n",
        "            if pd.notna(row['start']) and pd.notna(row['end']):\n",
        "                duration = (row['end'] - row['start']).total_seconds() / 60\n",
        "                duration = round(duration, 0)\n",
        "\n",
        "            location_val = str(row.get('location', '')).strip()\n",
        "            location = dict_locations.get(location_val, location_val)\n",
        "            stop_group_val = str(row.get('stopGroup', '')).strip()\n",
        "            stop_group = dict_stop_type.get(stop_group_val, stop_group_val)\n",
        "            stop_val = str(row.get('stop', '')).strip()\n",
        "            stop = dict_stop_reasons.get(stop_val, stop_val)\n",
        "            comment = str(row.get('comment', '')).strip()\n",
        "            combined = f\"{stop} ({comment})\".strip()\n",
        "\n",
        "            set_left_font48(sheet_814[f\"E{start_row}\"], start_time)\n",
        "            set_left_font48(sheet_814[f\"F{start_row}\"], end_time)\n",
        "            set_left_font48(sheet_814[f\"G{start_row}\"], duration)\n",
        "            set_left_font48(sheet_814[f\"H{start_row}\"], location)\n",
        "            set_left_font48(sheet_814[f\"K{start_row}\"], stop_group)\n",
        "            set_left_font48(sheet_814[f\"N{start_row}\"], combined)\n",
        "\n",
        "            start_row += 1\n",
        "\n",
        "    # 🔁 Speed losses API\n",
        "    speed_url = f\"https://api.evocon.com/api/reports/speedlosses_json?stationId={station_id}&startTime=2025-03-01&endTime=2050-01-01\"\n",
        "    response = requests.get(speed_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Speed losses API error: {response.status_code}\")\n",
        "\n",
        "    speed_df = pd.DataFrame(response.json())\n",
        "    speed_df['start'] = pd.to_datetime(speed_df['start'], errors='coerce')\n",
        "    speed_df['shiftName'] = speed_df['shiftName'].astype(str).str.strip()\n",
        "    speed_df['performanceLossNotes'] = speed_df['performanceLossNotes'].astype(str).fillna('').str.strip()\n",
        "    speed_df['stopMinutes'] = pd.to_numeric(speed_df['stopMinutes'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Filter by selected date and shift\n",
        "    if shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        speed_filtered = speed_df[\n",
        "            ((speed_df['start'].dt.date == selected_date) & (speed_df['start'].dt.time > time(21, 30))) |\n",
        "            ((speed_df['start'].dt.date == next_day) & (speed_df['start'].dt.time <= time(5, 30)))\n",
        "        ]\n",
        "    else:\n",
        "        speed_filtered = speed_df[\n",
        "            (speed_df['start'].dt.date == selected_date) &\n",
        "            (speed_df['shiftName'] == shift_label)\n",
        "        ]\n",
        "\n",
        "    # Group by comment (performanceLossNotes)\n",
        "    grouped_speed = speed_filtered.groupby('performanceLossNotes')\n",
        "\n",
        "    for comment_val, group in grouped_speed:\n",
        "        total_duration = round(group['stopMinutes'].sum(), 0)\n",
        "        comment = dict_speed_loss_reasons.get(comment_val, comment_val)\n",
        "\n",
        "        if start_row >= 49:\n",
        "            sheet_814.insert_rows(start_row)\n",
        "            for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                if merged_range.min_row == start_row:\n",
        "                    try:\n",
        "                        sheet_814.unmerge_cells(str(merged_range))\n",
        "                    except KeyError:\n",
        "                        pass\n",
        "\n",
        "            # --- Robustly copy all formatting from row before 49 to the new row ---\n",
        "            copy_row_format(sheet_814, 48, start_row)\n",
        "\n",
        "        # Write values to sheet\n",
        "        set_left(sheet_814[f\"E{start_row}\"], \"gubitak brzine\")\n",
        "        set_left(sheet_814[f\"F{start_row}\"], \"gubitak brzine\")\n",
        "        set_left(sheet_814[f\"G{start_row}\"], total_duration)\n",
        "        set_left(sheet_814[f\"K{start_row}\"], dict_speed_loss_group.get(\"Speed Loss\", \"Speed Loss\")) # Always \"Speed Loss\" group\n",
        "        set_left(sheet_814[f\"N{start_row}\"], comment)\n",
        "\n",
        "        start_row += 1\n",
        "\n",
        "\n",
        "\n",
        "    # 🚀 STEP: Fill product info (product name, SKU, start/end time of runs) into Sheet1\n",
        "\n",
        "\n",
        "    # 🧩 1. Get shift time boundaries\n",
        "    shift_start_time, shift_end_time = get_shift_bounds(shift, selected_date)\n",
        "\n",
        "    # 👀 2. Get first product info from the first stop\n",
        "    first_stop_row = filtered_losses.iloc[0]\n",
        "    first_product_name = str(first_stop_row.get(\"productName\", \"\")).strip()\n",
        "    first_product_sku = str(first_stop_row.get(\"productSku\", \"\")).strip()\n",
        "\n",
        "    # 🖋️ Fill F10 (SKU), F11 (product), and J10 (shift start time) initially\n",
        "    sheet_814[\"F10\"].value = first_product_sku\n",
        "    sheet_814[\"F11\"].value = first_product_name\n",
        "    sheet_814[\"J10\"].value = shift_start_time.strftime(\"%H:%M\")\n",
        "\n",
        "    # ⏳ 3. Fetch production runs and filter those within this shift\n",
        "    prod_url = f\"https://api.evocon.com/api/reports/production_runs_json?startTime=2025-03-01&endTime=2050-01-01&stationId={station_id}\"\n",
        "    prod_df = pd.DataFrame(requests.get(prod_url, headers=headers).json())\n",
        "\n",
        "    # Normalize times and dates\n",
        "    prod_df['startDate'] = pd.to_datetime(prod_df['startDate'], errors='coerce').dt.date\n",
        "    prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
        "    prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n",
        "\n",
        "    # For 3rd shift, we must span dates\n",
        "    if shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        shift_filter = (\n",
        "            ((prod_df['startDate'] == selected_date) & (prod_df['startTime'].dt.time >= shift_start_time)) |\n",
        "            ((prod_df['startDate'] == next_day) & (prod_df['startTime'].dt.time <= shift_end_time))\n",
        "        )\n",
        "    else:\n",
        "        shift_filter = (\n",
        "            (prod_df['startDate'] == selected_date) &\n",
        "            (prod_df['startTime'].dt.time >= shift_start_time) &\n",
        "            (prod_df['startTime'].dt.time <= shift_end_time)\n",
        "        )\n",
        "\n",
        "    prod_filtered = prod_df[shift_filter].sort_values(by=\"startTime\").reset_index(drop=True)\n",
        "\n",
        "    # 🧠 4. Build initial production run list from stop data\n",
        "    production_runs = [{\n",
        "        \"sku\": first_product_sku,\n",
        "        \"product\": first_product_name,\n",
        "        \"start_time\": shift_start_time.strftime(\"%H:%M\"),\n",
        "        \"end_time\": None  # to be filled later\n",
        "    }]\n",
        "\n",
        "    # 🔍 5. Detect \"Changeover and CIP\" stops to determine end/start transitions\n",
        "    changeover_rows = filtered_losses[filtered_losses[\"stopGroup\"] == \"Changeover and CIP\"].copy()\n",
        "    changeover_rows = changeover_rows.sort_values(by=\"start\").reset_index(drop=True)\n",
        "\n",
        "    # Assign end time of first run\n",
        "    if not changeover_rows.empty and pd.notna(changeover_rows.iloc[0][\"start\"]):\n",
        "        production_runs[0][\"end_time\"] = changeover_rows.iloc[0][\"start\"].strftime(\"%H:%M\")\n",
        "    else:\n",
        "        production_runs[0][\"end_time\"] = shift_end_time.strftime(\"%H:%M\")\n",
        "\n",
        "\n",
        "    # 🔁 6. Append production runs from prod_filtered (with proper changeover sequencing)\n",
        "    changeover_rows = changeover_rows.reset_index(drop=True)  # Safe for iloc\n",
        "    all_stops = filtered_losses.reset_index(drop=True)        # Ensure same for full stops list\n",
        "\n",
        "    changeover_idx = 0\n",
        "    last_valid_end = None\n",
        "\n",
        "    for i, row in prod_filtered.iterrows():\n",
        "        sku = str(row.get(\"sku\", \"\")).strip()\n",
        "        product = str(row.get(\"product\", \"\")).strip()\n",
        "\n",
        "        # 🧩 Find the last \"Changeover and CIP\" in a consecutive sequence\n",
        "        cip_end_time = None\n",
        "        while changeover_idx < len(changeover_rows):\n",
        "            current_cip = changeover_rows.iloc[changeover_idx]\n",
        "            changeover_idx += 1\n",
        "\n",
        "            # If last or next stop is not \"Changeover and CIP\", this is the last in sequence\n",
        "            if changeover_idx == len(changeover_rows):\n",
        "                cip_end_time = current_cip.get(\"end\")\n",
        "                break\n",
        "\n",
        "            next_cip = changeover_rows.iloc[changeover_idx]\n",
        "            curr_idx_in_all = all_stops[all_stops['start'] == current_cip['start']].index[0]\n",
        "\n",
        "            if curr_idx_in_all + 1 < len(all_stops):\n",
        "                next_stop_group = all_stops.iloc[curr_idx_in_all + 1].get(\"stopGroup\", \"\")\n",
        "                if next_stop_group != \"Changeover and CIP\":\n",
        "                    cip_end_time = current_cip.get(\"end\")\n",
        "                    break\n",
        "\n",
        "        # 🕒 Start time = end of last CIP, or start of shift\n",
        "        if pd.notna(cip_end_time):\n",
        "            start_time = cip_end_time\n",
        "        else:\n",
        "            start_time = row.get(\"startTime\") or shift_start_time\n",
        "\n",
        "        start_str = start_time.strftime(\"%H:%M\") if pd.notna(start_time) else \"\"\n",
        "\n",
        "        # 🕛 End time = next CIP start, or shift end\n",
        "        if changeover_idx < len(changeover_rows):\n",
        "            next_cip_start = changeover_rows.iloc[changeover_idx][\"start\"]\n",
        "            end_str = next_cip_start.strftime(\"%H:%M\") if pd.notna(next_cip_start) else shift_end_time.strftime(\"%H:%M\")\n",
        "        else:\n",
        "            end_str = shift_end_time.strftime(\"%H:%M\")\n",
        "\n",
        "        production_runs.append({\n",
        "            \"sku\": sku,\n",
        "            \"product\": product,\n",
        "            \"start_time\": start_str,\n",
        "            \"end_time\": end_str\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #version of insert rows and writing with font and size formatting and with row height adjustment\n",
        "\n",
        "\n",
        "    # 🔠 Font config\n",
        "    prod_font = Font(name=\"Arial\", size=48)\n",
        "    bold_font = Font(bold=True)\n",
        "\n",
        "    # 🧱 Insert new rows (if more than one product)\n",
        "    insert_point = 10\n",
        "    extra_runs = len(production_runs) - 1\n",
        "\n",
        "    if extra_runs > 0:\n",
        "        sheet_814.insert_rows(insert_point, amount=extra_runs * 2)\n",
        "        source_row = insert_point - 1\n",
        "        for i in range(extra_runs * 2):\n",
        "            target_row = insert_point + i\n",
        "            copy_row_format(sheet_814, source_row, target_row)\n",
        "    # 🧾 content to be written in new cells inserted\n",
        "    title_map = {\n",
        "        \"E\": \"Šifra:\",\n",
        "        \"E_lower\": \"Naziv proizvoda:\",\n",
        "        \"H\": \"Proizvodnja od (prva boca kontinuirano na punjaču):\",\n",
        "        \"H_lower\": \"Proizvodnja do (zadnja boca na punjaču):\",\n",
        "    }\n",
        "\n",
        "    # 🧬 Apply values and formatting row by row\n",
        "    for i, run in enumerate(production_runs):\n",
        "        row_top = 10 + (i * 2)\n",
        "        row_bottom = row_top + 1\n",
        "\n",
        "        # Copy titles to column E and H\n",
        "        sheet_814[f\"E{row_top}\"].value = title_map[\"E\"]\n",
        "        sheet_814[f\"E{row_bottom}\"].value = title_map[\"E_lower\"]\n",
        "        sheet_814[f\"H{row_top}\"].value = title_map[\"H\"]\n",
        "        sheet_814[f\"H{row_bottom}\"].value = title_map[\"H_lower\"]\n",
        "\n",
        "        for col_letter in ['E', 'F', 'H', 'J']:\n",
        "            for r in [row_top, row_bottom]:\n",
        "                cell = sheet_814[f\"{col_letter}{r}\"]\n",
        "                cell.font = prod_font\n",
        "                cell.alignment = Alignment(horizontal='left', wrap_text=True)\n",
        "\n",
        "        # Fill values for product info\n",
        "        sheet_814[f\"F{row_top}\"].value = run[\"sku\"]\n",
        "        sheet_814[f\"F{row_bottom}\"].value = run[\"product\"]\n",
        "        sheet_814[f\"J{row_top}\"].value = run[\"start_time\"]\n",
        "        sheet_814[f\"J{row_bottom}\"].value = run[\"end_time\"]\n",
        "        # ovdje je u og kodu bilo height = 159\n",
        "        # Dynamically adjust row height based on the maximum number of lines in the cells\n",
        "        def get_max_lines(row_num, columns):\n",
        "            max_lines = 1\n",
        "            for col in columns:\n",
        "                cell_value = str(sheet_814[f\"{col}{row_num}\"].value or \"\")\n",
        "                lines = cell_value.count('\\n') + 1\n",
        "                max_lines = max(max_lines, lines)\n",
        "            return max_lines\n",
        "\n",
        "        font_size = prod_font.size if hasattr(prod_font, 'size') else 12\n",
        "        line_height = font_size * 3.6  # Approximate line height\n",
        "        for r in [row_top, row_bottom]:\n",
        "            max_lines = get_max_lines(r, ['E', 'F', 'H', 'J'])\n",
        "            sheet_814.row_dimensions[r].height = max(30, int(line_height * max_lines))\n",
        "\n",
        "    header = 30 #gornja sastavnica\n",
        "\n",
        "    def make_header(from_sheet, header_rows):\n",
        "    #from_sheet se samo referira na sheet excela\n",
        "    # postavljanje headera kao rows\n",
        "        from_sheet.print_title_rows = f'1:{header_rows}'\n",
        "\n",
        "    make_header(sheet_814, header)\n",
        "    # 💾 Step 11: Save and download\n",
        "    from openpyxl.utils import get_column_letter\n",
        "\n",
        "# namjesti print area na cijeli range\n",
        "# jer ako netko ne klikne \"ignore print range\" pri printanju\n",
        "# nece iskopirati sve redove\n",
        "    max_row = sheet_814.max_row\n",
        "    max_col = sheet_814.max_column\n",
        "    print_area = f\"A1:{get_column_letter(max_col)}{max_row}\"\n",
        "    sheet_814.print_area = print_area\n",
        "\n",
        "# --- New code to check and adjust row 13 height ---\n",
        "# Calculate the current row number corresponding to the original row 13\n",
        "# It will be 13 + (number of extra runs * 2)\n",
        "    current_row_13 = 13 + (extra_runs * 2)\n",
        "\n",
        "# --- New code to check and adjust row 13 height ---\n",
        "# Calculate the current row number corresponding to the original row 13\n",
        "# It will be 13 + (number of extra runs * 2)\n",
        "    current_row_12 = 12 + (extra_runs * 2)\n",
        "\n",
        "# Check if the row exists in row_dimensions and its height is less than 90\n",
        "    if current_row_12 in sheet_814.row_dimensions:\n",
        "        current_height = sheet_814.row_dimensions[current_row_12].height\n",
        "    # openpyxl stores default height as None, explicitly check against None\n",
        "        if current_height is None or current_height < 90:\n",
        "            sheet_814.row_dimensions[current_row_12].height = 90\n",
        "    else:\n",
        "    # If the row doesn't have an entry in row_dimensions, it has default height.\n",
        "    # Explicitly set it to 90 if it's less than 90 (default is usually less)\n",
        "        sheet_814.row_dimensions[current_row_12].height = 90\n",
        "\n",
        "    current_row_13 = 13 + (extra_runs * 2)\n",
        "    # Check if the row exists in row_dimensions and its height is less than 90\n",
        "    if current_row_13 in sheet_814.row_dimensions:\n",
        "        current_height = sheet_814.row_dimensions[current_row_13].height\n",
        "    # openpyxl stores default height as None, explicitly check against None\n",
        "        if current_height is None or current_height < 90:\n",
        "            sheet_814.row_dimensions[current_row_13].height = 90\n",
        "    else:\n",
        "    # If the row doesn't have an entry in row_dimensions, it has default height.\n",
        "    # Explicitly set it to 90 if it's less than 90 (default is usually less)\n",
        "        sheet_814.row_dimensions[current_row_13].height = 90\n",
        "\n",
        "    # ❌ Disable \"Fit to Page\"\n",
        "    #ws.page_setup.fitToWidth = 0\n",
        "    ws.page_setup.fitToHeight = 0\n",
        "    ws.page_setup.scale = 20  # keep original scale (no shrinking)\n",
        "\n",
        "    # ✅ Set paper size and orientation if needed\n",
        "    ws.page_setup.paperSize = ws.PAPERSIZE_A4\n",
        "    ws.page_setup.orientation = ws.ORIENTATION_LANDSCAPE\n",
        "\n",
        "    output_file = f\"smjenski_izvjestaj_{line_input}_smjena_{shift}_{date_str}.xlsx\"\n",
        "    #wb.save(full_path)\n",
        "    wb.save(output_file)\n",
        "    # Create the folder (if it doesn't exist)\n",
        "    os.makedirs(\"excel_reports\", exist_ok=True)\n",
        "\n",
        "    # Move the file to the output directory\n",
        "    os.replace(output_file, os.path.join(\"excel_reports\", output_file))\n",
        "    #files.download(output_file)\n",
        "\"\"\"\n",
        "        # Git commit and push\n",
        "    try:\n",
        "      print(\"---GIT ADD---\")\n",
        "      subprocess.run([\"git\", \"add\", str(full_path)], check=True, capture_output=True, text=True)\n",
        "      print(\"---GIT COMMIT---\")\n",
        "      subprocess.run([\"git\", \"commit\", \"-m\", f\"Add Excel file {filename}\"], check=True, capture_output=True, text=True)\n",
        "      print(\"---GIT PUSH---\")\n",
        "      subprocess.run([\"git\", \"push\"], check=True, capture_output=True, text=True)\n",
        "      print(\"✅ Excel file committed and pushed to GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "      print(f\"❌ Git operation failed: {e}\")\n",
        "      print(f\"---STDOUT---:\\n{e.stdout}\")\n",
        "      print(f\"---STDERR---:\\n{e.stderr}\")\n",
        "\"\"\""
      ]
    }
  ]
}