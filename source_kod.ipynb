{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKpnmVCp1Luw50p/jKsZiP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smjenski-izvjestaji/izvjestaji/blob/main/source_kod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "1vsVZYiqhzk4",
        "outputId": "dd8f65e6-14b0-40fa-c844-6284e4d0cf71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Processing line: RGB (Station ID: 1)\n",
            "‚úÖ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4122486230.py:565: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-4122486230.py:566: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_22fa3d67-c541-4758-89a2-cf0b8e649476\", \"updated_shift_report.xlsx\", 10726)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: PET1 (Station ID: 2)\n",
            "‚úÖ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4122486230.py:565: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-4122486230.py:566: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9421e2dd-a28c-4b49-917c-54fd9e92c117\", \"updated_shift_report.xlsx\", 10676)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: PET2 (Station ID: 3)\n",
            "‚úÖ Template downloaded: template.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4122486230.py:565: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
            "/tmp/ipython-input-4122486230.py:566: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aee388c6-a54e-4b63-8a2c-727e912292b3\", \"updated_shift_report.xlsx\", 10687)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install pandas openpyxl requests\n",
        "\n",
        "# Step 2: Upload the Excel template\n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import time as systime\n",
        "import ast\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.cell.cell import MergedCell\n",
        "from openpyxl.utils import get_column_letter\n",
        "from datetime import datetime, time\n",
        "from openpyxl.styles import Alignment, Font, Border  # remove when going back to .py version\n",
        "from copy import copy\n",
        "\n",
        "import requests\n",
        "\n",
        "from datetime import datetime, time\n",
        "from openpyxl import Workbook\n",
        "\n",
        "from datetime import datetime, time, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "\n",
        "\n",
        "import openpyxl\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "\n",
        "#from google.colab import files\n",
        "\n",
        "dict_stop_reasons={\"Electrical Failure\":\"Elektriƒçni kvar\", \"Bottle Necks\":\"Usko grlo\", \"COP\":\"COP\", \"End of production\":\"Zavr≈°etak proizvodnje\", \"Not recorded\":\"Nije zabilje≈æen\", \"Preventive Maintenance\":\"Preventivno odr≈æavanje\", \"Changeover\":\"Changeover\", \"Other failure\":\"Drugi kvar\", \"Adjustments on equipment\":\"Prilagodbe na opremi\", \"CIP (5ST)\":\"CIP (5ST)\", \"Mechanical failure\":\"Mehaniƒçki kvar\",\"Meeting\":\"Sastanak\", \"Start of production\":\"Poƒçetak proizvodnje\", \"CIP (3V)\":\"CIP (3V)\", \"Cleaning on line\":\"ƒåi≈°ƒáenje na liniji\", \"CIP (3L)\":\"CIP (3L)\", \"Insufficient Electrical en.supply\":\"Nedovoljna opskrba elektr. energije\",\"CIP (OV)\":\"CIP (OV)\",\"Insufficient Water supply\":\"Nedovoljna opskrba vode\",\"Lack of personnel\":\"Nedostatak operatera\",\"Changeover + CIP (3V)\":\"Changeover + CIP (3V)\",\"Not skilled personnel\":\"Nekvalificirani operateri\",\"Changeover + CIP (3L)\":\"Changeover + CIP (3L)\",\"Other\":\"Drugo\",\"Changeover + CIP (OV)\":\"Changeover + CIP (OV)\",\"QA tests on the line\":\"QA testovi na liniji\",\"Test bottle\":\"Probna boca\",\"Falling bottles on conveyors\":\"Padaju boce na transporterima\",\"P1 (CHO 12/24)\":\"P1 (CHO 12/24)\",\"Stopped production from QA\":\"Zaustavljena proizvodnja iz QA\",\"P2 (CHO filler)\":\"P2 (CHO filler)\",\"Insufficient CO2 supply\":\"Nedovoljna opskrba CO2\",\"P3 (CHO+OV+MIX)\":\"P3 (CHO+OV+MIX)\",\"Waiting for the Syrup\":\"ƒåekanje sirupa\",\"P4 (CHO+3¬∞+MIX)\":\"P4 (CHO+3¬∞+MIX)\",\"Insufficient Cold water supply\":\"Nedovoljna opskrba hladne vode\",\"P5 (CHO+5¬∞+MIX)\":\"P5 (CHO+5¬∞+MIX)\",\"Poor CHO performance\":\"Lo≈° performans changeovera\",\"P6 (OV+MIX)\":\"P6 (OV+MIX)\",\"Poor Bottles Quality\":\"Lo≈°a kvaliteta boca\",\"P7 (3¬∞+MIX)\":\"P7 (3¬∞+MIX)\",\"Poor Crates Quality\":\"Lo≈°a kvaliteta sanduka\",\"P8 (5¬∞+MIX)\":\"P8 (5¬∞+MIX)\",\"Poor Labels Quality\":\"Lo≈°a kvaliteta etiketa\",\"P9 (DZ+OV+MIX)\":\"P9 (DZ+OV+MIX)\",\"Poor Pallets (Wood) Quality\":\"Lo≈°a kvaliteta drvenih paleta\",\"P10 (DZ+3¬∞+MIX)\":\"P10 (DZ+3¬∞+MIX)\",\"Poor Cap Quality\":\"Lo≈°a kvaliteta zatvaraƒça\",\"P11 (DZ+5¬∞+MIX)\":\"P11 (DZ+5¬∞+MIX)\",\"Poor support from Storage\":\"Lo≈°a podr≈°ka iz skladi≈°ta\",\"Poor lubrication of conveyors\":\"Slabo podmazivanje transportera\",\"Poor thermofoil quality\":\"Lo≈°a kvaliteta termofolije\",\"Poor Preform Quality\":\"Lo≈°a kvaliteta predforme\",\"Replenishment of raw materials\":\"Nadopuna repromaterijala\",\"Shift break\":\"Smjenska pauza\",\"Mixed crates\":\"Mije≈°ani sanduci\",\"Mixed bottles\":\"Mije≈°ane boce\",\"Dirty crates\":\"Prljavi sanduci\",\"Dirty bottles\":\"Prljave boce\",\"test\":\"test\",\"Uncommented\":\"Bez komentara\"}\n",
        "dict_stop_type={\"EPL\":\"EPL\",\"OPL\":\"OPL\",\"Non scheduled time\":\"Neplanirano vrijeme\",\"Changeover and CIP\":\"Changeover i CIP\",\"Maintenance and Set up\":\"Odr≈æavanje i setup\",\"Minor Stoppages\":\"Manji zastoji\",\"Preventive Maintenance\":\"Preventivno odr≈æavanje\",\"Speed Losses\":\"Gubici brzine\",\"Uncommented\":\"Bez komentara\"}\n",
        "dict_speed_loss_reasons={\"Speed loss\":\"Gubitak brzine\"}\n",
        "dict_speed_loss_group={\"Speed Loss\":\"Gubitak brzine\"}\n",
        "dict_locations={\"Infeed of caser\":\"Ulaz u upakivaƒç\",\"Crates conveyors from caser to palletizer\":\"Transporteri sanduka od upakivaƒça do paletizera\",\"Bottles conveyor from uncaser to washer machine\":\"Transporter boca od ispakivaƒça do perilice\",\"Bottle washer\":\"Perilica boca\",\"Bottles conveyor from washer to filler\":\"Transporter boca od perilice do punjaƒça\",\"Linatronic\":\"Linatronic\",\"Mixer\":\"Mixer\",\"Bottles conveyor from filler to labelling machine\":\"Transporter boca od punjaƒça do etiketirke\",\"Bottles conveyor from labelling machine to caser\":\"Transporter boca od etiketirke do upakivaƒça\",\"Caser\":\"Upakivaƒç\",\"Crates conveyors from depalletizer to uncaser\":\"Transporteri sanduka od depaletizera do ispakivaƒça\",\"Uncaser\":\"Ispakivaƒç\",\"Infeed of bottle washer\":\"Ulaz u perilicu boca\",\"Outfeed from washer machine\":\"Izlaz iz perilice boca\",\"Inliner in white line\":\"Inliner u bijeloj liniji\",\"Cap conveyors\":\"Transporteri zatvaraƒça\",\"Checkmat after filler\":\"Checkmat nakon punjaƒça\",\"Depalletizer\":\"Depaletizer\",\"Filler\":\"Punjaƒç\",\"Palletizer\":\"Paletizer\",\"Labeller\":\"Etiketirka\",\"Crates conveyor from uncaser to washer\":\"Transporter sanduka od ispakivaƒça do perilice sanduka\",\"Crates conveyor from washer to caser\":\"Transporter sanduka od perilice sanduka do upakivaƒça\",\"Crates washer\":\"Perilica sanduka\",\"Packer (Zambelli)\":\"Pakiralica (Zambelli)\",\"Blower\":\"Puhalica\",\"Packer (KHS)\":\"Pakiralica (KHS)\",\"Handle machine\":\"Ruƒçkomat\",\"Air conveyor\":\"Zraƒçni transporter\",\"Conveyor from labeller to packer (KHS)\":\"Transporter od etiketirke do pakiralice (KHS)\",\"Conveyor from KHS to handle machine\":\"Transporter od KHS do ruƒçkomata\",\"Conveyor from handle machine to Zambelli\":\"Transporter od ruƒçkomata do Zambelli-a\",\"Conveyor from packer (Zambelli) to palletizer\":\"Transporter od pakiralice (Zambelli) do paletizera\",\"Conveyor from labeller to packer (Zambelli)\":\"Transporter od etiketirke do pakiralice (Zambelli)\",\"Entire line\":\"Cijela linija\",\"Water treatment\":\"Obrada vode\",\"Syrup room\":\"Sirupana\",\"Compressor room\":\"Kompresorska stanica\",\"Cooler\":\"Hladnjak\",\"Rinser\":\"Ispiraƒçica\",\"Capper\":\"Zatvaraƒçica\",\"Date printer\":\"Datumar\",\"Case sticker applicator\":\"Aplikator naljepnica na paket\",\"Pallet sticker applicator\":\"Aplikator naljepnica na paletu\",\"Foam maker\":\"Pjenomat\"}\n",
        "\n",
        "# Station mapping\n",
        "station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "\n",
        "# Replace with your actual timezone (e.g., 'Europe/Belgrade', 'Europe/Zagreb')\n",
        "local_timezone = ZoneInfo('Europe/Zagreb')\n",
        "\n",
        "# Get local time\n",
        "now = datetime.utcnow()\n",
        "now_local = now.replace(tzinfo=ZoneInfo('UTC')).astimezone(local_timezone)\n",
        "\n",
        "current_time = now_local.time()\n",
        "\"\"\"\n",
        "# Determine shift and date based on local time\n",
        "# vrijeme je u UTC+0 by default a ne CEST (UTC+2) PAZI!!!\n",
        "# zato smo uveli now_local\n",
        "if time(5, 30) <= current_time < time(13, 30):\n",
        "    shift = 1\n",
        "    selected_date = now_local.date()\n",
        "elif time(13, 30) <= current_time < time(21, 30):\n",
        "    shift = 2\n",
        "    selected_date = now_local.date()\n",
        "else:\n",
        "    shift = 3\n",
        "    selected_date = (now_local - timedelta(days=1)).date()  # Use previous day\n",
        "\"\"\"\n",
        "\n",
        "# Determine shift and date based on local time\n",
        "# vrijeme je u UTC+0 by default a ne CEST (UTC+2) PAZI!!!\n",
        "# zato smo uveli now_local\n",
        "if time(6, 40) <= current_time < time(14, 40):\n",
        "    shift = 1\n",
        "    selected_date = now_local.date()\n",
        "elif time(14, 40) <= current_time < time(22, 40):\n",
        "    shift = 2\n",
        "    selected_date = now_local.date()\n",
        "else:\n",
        "    shift = 3\n",
        "    selected_date = (now_local - timedelta(days=1)).date()  # Use previous day\n",
        "\n",
        "\n",
        "\n",
        "# Format date for filename\n",
        "date_str = selected_date.strftime('%Y%m%d')\n",
        "\n",
        "#output_dir = Path(\"excel_reports\")\n",
        "#output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Iterate over each production line\n",
        "for line_input, station_id in station_map.items():\n",
        "    print(f\"Processing line: {line_input} (Station ID: {station_id})\")\n",
        "\n",
        "\n",
        "    # GitHub raw URL to the Excel template\n",
        "    template_url = \"https://raw.githubusercontent.com/smjenski-izvjestaji/izvjestaji/main/Shift_report_template.xlsx\"\n",
        "    filename = \"template.xlsx\"\n",
        "\n",
        "    # üì• Download the file\n",
        "    response = requests.get(template_url)\n",
        "    if response.status_code == 200:\n",
        "      with open(filename, 'wb') as f:\n",
        "          f.write(response.content)\n",
        "      print(f\"‚úÖ Template downloaded: {filename}\")\n",
        "    else:\n",
        "      raise Exception(f\"‚ùå Failed to download file: {response.status_code}\")\n",
        "\n",
        "    # üß† Step 3: Load workbook and sheet\n",
        "    #filename = next(iter(uploaded))\n",
        "    wb = load_workbook(filename)\n",
        "    #systime.sleep(10)  # Optional delay\n",
        "    sheet_814 = wb['Sheet1']\n",
        "\n",
        "    #api_key = os.getenv(\"API_KEY\")\n",
        "    #secret_key = os.getenv(\"SECRET_KEY\")\n",
        "\n",
        "# üîê Step 4: Prepare API credentials\n",
        "    api_key = \"EVO99B7A17C6CF2410\"         # üîÅ Replace with your API key\n",
        "    secret_key = \"v2xLnrFQD18WYtyUEykK\"   # üîÅ Replace with your secret key\n",
        "\n",
        "\n",
        "    # üîê Step 4: Prepare API credentials\n",
        "    #api_key = \"API_KEY\"         # üîÅ Replace with your API key\n",
        "    #secret_key = \"SECRET_KEY\"   # üîÅ Replace with your secret key\n",
        "\n",
        "    credentials = f\"{api_key}:{secret_key}\"\n",
        "    encoded_credentials = base64.b64encode(credentials.encode()).decode()\n",
        "\n",
        "    headers = {\n",
        "    \"Authorization\": f\"Basic {encoded_credentials}\"\n",
        "    }\n",
        "\n",
        "    station_id = station_map[line_input]\n",
        "\n",
        "    \"\"\"\n",
        "    # üè≠ Ask user for production line\n",
        "    line_input = input(\"Enter production line (PET1, PET2, or RGB): \").strip().upper().replace(\" \", \"\")\n",
        "    station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "\n",
        "    if line_input not in station_map:\n",
        "    raise ValueError(\"Invalid line. Use RGB, PET1, or PET2.\")\n",
        "\n",
        "    station_id = station_map[line_input]\n",
        "\n",
        "    # üìÖ Ask user for date\n",
        "    selected_date_input = input(\"Enter the date to retrieve checklists and stops (DD.MM.YYYY): \").strip()\n",
        "    try:\n",
        "        selected_date = datetime.strptime(selected_date_input, \"%d.%m.%Y\").date()\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Invalid date format. Please use DD.MM.YYYY.\")\n",
        "\n",
        "\n",
        "    # üë• Ask user for shift number\n",
        "    shift_input = input(\"Enter shift number (1, 2, or 3): \").strip()\n",
        "    if shift_input not in ['1', '2', '3']:\n",
        "        raise ValueError(\"Invalid shift. Please enter 1, 2, or 3.\")\n",
        "    shift = int(shift_input)\n",
        "    shift_label = {1: \"1st\", 2: \"2nd\", 3: \"3rd\"}[shift]\n",
        "    \"\"\"\n",
        "\n",
        "    # Station map\n",
        "    station_map = {\"RGB\": 1, \"PET1\": 2, \"PET2\": 3}\n",
        "    \"\"\"\n",
        "    # üìÖ Automatically use today's date\n",
        "    selected_date = datetime.today().date()\n",
        "\n",
        "    # ‚è∞ Automatically determine shift based on current time\n",
        "    current_time = datetime.now().time()\n",
        "\n",
        "    if time(5, 30) <= current_time < time(13, 30):\n",
        "        shift = 1\n",
        "    elif time(13, 30) <= current_time < time(21, 30):\n",
        "        shift = 2\n",
        "    else:\n",
        "        shift = 3\n",
        "    \"\"\"\n",
        "    shift_label = {1: \"1st\", 2: \"2nd\", 3: \"3rd\"}[shift]\n",
        "\n",
        "\n",
        "    #  Define shift time windows for start and end\n",
        "    def get_shift_bounds(shift, date):\n",
        "        if shift == 1:\n",
        "            return time(5, 30), time(13, 30)\n",
        "        elif shift == 2:\n",
        "            return time(13, 30), time(21, 30)\n",
        "        elif shift == 3:\n",
        "            # Third shift spans 2 dates\n",
        "            return time(21, 30), time(5, 30)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid shift number\")\n",
        "\n",
        "#\n",
        "    # üßæ Write date and shift to cells D9 and F9\n",
        "    def set_left(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='left')\n",
        "\n",
        "    def set_left_font48(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='left')\n",
        "        cell.font = Font(name=\"Arial\", size=48)\n",
        "\n",
        "    def set_center(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "    def set_right(cell, value):\n",
        "        cell.value = value\n",
        "        cell.alignment = Alignment(horizontal='right')\n",
        "\n",
        "    set_left(sheet_814[\"D9\"], selected_date.strftime(\"%d.%m.%Y\"))\n",
        "    set_left(sheet_814[\"F9\"], str(shift))\n",
        "\n",
        "    # write other info in header (punjac, linija, ...)\n",
        "\n",
        "    if line_input == \"PET1\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjaƒç O+H\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE PET BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 866 (PET1)\")\n",
        "\n",
        "    elif line_input == \"PET2\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjaƒç Krones\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE PET BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 814 (PET2)\")\n",
        "\n",
        "    elif line_input == \"RGB\":\n",
        "        set_center(sheet_814[\"C6\"], \"Punjaƒç Krones\")\n",
        "        set_center(sheet_814[\"E6\"], \"PUNJENJE RGB BOCA\")\n",
        "        set_center(sheet_814[\"Q6\"], \"LINIJA 865 (RGB)\")\n",
        "\n",
        "\n",
        "\n",
        "    # üì° Step 5: Fetch checklist data from API\n",
        "    checklist_url = f\"https://api.evocon.com/api/reports/checklists_json?startTime=2025-03-01&endTime=2050-01-01&stationId={station_id}\"\n",
        "    response = requests.get(checklist_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "      raise Exception(f\"Checklist API error {response.status_code}: {response.text}\")\n",
        "\n",
        "    checklist_df = pd.DataFrame(response.json())\n",
        "\n",
        "    # üßæ Step 6: Normalize checklist data\n",
        "    def clean_time(t):\n",
        "        if pd.isna(t): return None\n",
        "        t = str(t).replace(\":\", \"\").strip()\n",
        "        return t.zfill(4)[-4:]\n",
        "\n",
        "    # Handle stringified lists in 'itemresult'\n",
        "    def extract_single_value(x):\n",
        "        if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(x)\n",
        "                if isinstance(parsed, list) and len(parsed) == 1:\n",
        "                    return str(parsed[0])\n",
        "            except:\n",
        "                return x\n",
        "        return str(x).strip()\n",
        "\n",
        "    checklist_df['clean_duetime'] = checklist_df['duetime'].apply(clean_time)\n",
        "    checklist_df['duetime_dt'] = pd.to_datetime(checklist_df['duetime'], format='%H:%M', errors='coerce').dt.time\n",
        "    checklist_df['date'] = pd.to_datetime(checklist_df['date'], errors='coerce').dt.date\n",
        "    checklist_df['itemname'] = checklist_df['itemname'].astype(str).str.strip().str.lower()\n",
        "    checklist_df['result'] = checklist_df['result'].astype(str).str.strip().str.lower()\n",
        "    checklist_df['itemresult'] = checklist_df['itemresult'].apply(extract_single_value)\n",
        "\n",
        "    # Apply shift filter\n",
        "    if shift == 1:\n",
        "        time_filter = (checklist_df['date'] == selected_date) & \\\n",
        "                      (checklist_df['duetime_dt'] >= time(6, 0)) & \\\n",
        "                      (checklist_df['duetime_dt'] <= time(14, 0))\n",
        "    elif shift == 2:\n",
        "        time_filter = (checklist_df['date'] == selected_date) & \\\n",
        "                      (checklist_df['duetime_dt'] > time(14, 0)) & \\\n",
        "                      (checklist_df['duetime_dt'] <= time(22, 0))\n",
        "    elif shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        time_filter = (\n",
        "            ((checklist_df['date'] == selected_date) & (checklist_df['duetime_dt'] > time(22, 0))) |\n",
        "            ((checklist_df['date'] == next_day) & (checklist_df['duetime_dt'] <= time(5, 46)))\n",
        "        )\n",
        "\n",
        "    checklist_df = checklist_df[time_filter]\n",
        "\n",
        "    # Map parameter names from D17‚ÄìD27\n",
        "    param_row_map = {}\n",
        "    for row in range(17, 28):\n",
        "        val = sheet_814[f\"D{row}\"].value\n",
        "        if val:\n",
        "            param_row_map[val.strip().lower()] = row\n",
        "\n",
        "    # Match headers in row 16\n",
        "    header_map = {}\n",
        "    for col in sheet_814.iter_cols(min_row=16, max_row=16):\n",
        "        cell = col[0]\n",
        "        if cell.value:\n",
        "            values = [p.strip().replace(\":\", \"\") for p in str(cell.value).split('/')]\n",
        "            if len(values) == 3:\n",
        "                header_map[col[0].column_letter] = {\n",
        "                    'morning': values[0].zfill(4),\n",
        "                    'afternoon': values[1].zfill(4),\n",
        "                    'night': values[2].zfill(4)\n",
        "                }\n",
        "\n",
        "    # Write checklist values\n",
        "    for _, row in checklist_df.iterrows():\n",
        "        if row['result'] != 'successful':\n",
        "            continue\n",
        "\n",
        "        param_name = row['itemname']\n",
        "        result = row['itemresult']\n",
        "        duetime = row['duetime_dt']\n",
        "        duetime_str = row['clean_duetime']\n",
        "\n",
        "        if pd.isna(duetime) or param_name not in param_row_map:\n",
        "            continue\n",
        "\n",
        "        shift_key = None\n",
        "        if time(6, 15) <= duetime <= time(13, 45):\n",
        "            shift_key = 'morning'\n",
        "        elif time(14, 15) <= duetime <= time(21, 45):\n",
        "            shift_key = 'afternoon'\n",
        "        elif duetime >= time(22, 15) or duetime <= time(5, 45):\n",
        "            shift_key = 'night'\n",
        "\n",
        "        target_column = None\n",
        "        for col_letter, shift_times in header_map.items():\n",
        "            if shift_times.get(shift_key) == duetime_str:\n",
        "                target_column = col_letter\n",
        "                break\n",
        "\n",
        "        if not target_column:\n",
        "            continue\n",
        "\n",
        "        row_number = param_row_map[param_name]\n",
        "        cell = sheet_814[f\"{target_column}{row_number}\"]\n",
        "        if not isinstance(cell, MergedCell):\n",
        "            set_left(cell, result)\n",
        "\n",
        "    # üåê API 2: Stop reasons\n",
        "    losses_url = f\"https://api.evocon.com/api/reports/losses_json?stationId={station_id}&startTime=2025-03-01&endTime=2050-01-01\"\n",
        "    response = requests.get(losses_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Losses API error: {response.status_code}\")\n",
        "\n",
        "    losses_df = pd.DataFrame(response.json())\n",
        "\n",
        "    losses_df['shiftStart'] = pd.to_datetime(losses_df['shiftStart'], errors='coerce')\n",
        "    losses_df['start'] = pd.to_datetime(losses_df['start'], errors='coerce')\n",
        "    losses_df['end'] = pd.to_datetime(losses_df['end'], errors='coerce')\n",
        "\n",
        "    filtered_losses = losses_df[\n",
        "        (losses_df['shiftStart'].dt.date == selected_date) &\n",
        "        (losses_df['shiftName'] == shift_label)\n",
        "    ]\n",
        "\n",
        "# Write stops to 814 starting from row 32\n",
        "# New scrit with joined stops\n",
        "\n",
        "    start_row = 32\n",
        "    processed_join_ids = set()\n",
        "\n",
        "    for idx, row in filtered_losses.iterrows():\n",
        "        join_id = row.get('stopJoinId')\n",
        "\n",
        "        if join_id and str(join_id).strip() != \"\":\n",
        "            if join_id in processed_join_ids:\n",
        "                continue\n",
        "            processed_join_ids.add(join_id)\n",
        "\n",
        "            # ‚úÖ get full join group from losses_df (not just filtered)\n",
        "            group = losses_df[losses_df['stopJoinId'] == join_id].copy()\n",
        "            group = group.sort_values(by='start')\n",
        "\n",
        "            total_duration = 0\n",
        "            for _, sub in group.iterrows():\n",
        "                if pd.notna(sub['start']) and pd.notna(sub['end']):\n",
        "                    total_duration += (sub['end'] - sub['start']).total_seconds() / 60\n",
        "            total_duration = round(total_duration, 0)\n",
        "\n",
        "\n",
        "            location = dict_locations.get(group['location'].dropna().astype(str).str.strip().iloc[0], group['location'].dropna().astype(str).str.strip().iloc[0] if not group['location'].dropna().empty else '')\n",
        "            stop_group_val = group['stopGroup'].dropna().astype(str).str.strip().iloc[0] if not group['stopGroup'].dropna().empty else ''\n",
        "            stop_group = dict_stop_type.get(stop_group_val, stop_group_val)\n",
        "            stop_val = str(group['stop'].iloc[0]).strip()\n",
        "            stop = dict_stop_reasons.get(stop_val, stop_val)\n",
        "            comment = str(group['comment'].iloc[0]).strip()\n",
        "            combined = f\"{stop} ({comment})\".strip()\n",
        "\n",
        "            if start_row >= 49:\n",
        "                sheet_814.insert_rows(start_row)\n",
        "\n",
        "                # üßΩ Unmerge any cells that were auto-merged in the inserted row\n",
        "                for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                    if merged_range.min_row == start_row:\n",
        "                        try:\n",
        "                            sheet_814.unmerge_cells(str(merged_range))\n",
        "                        except KeyError:\n",
        "                            pass # merged cell may already have been cleared\n",
        "\n",
        "                # üé® Copy style from row 49 to the new row\n",
        "\n",
        "                for col in range(1, sheet_814.max_column + 1):\n",
        "                    source_cell = sheet_814.cell(row=49, column=col)\n",
        "                    target_cell = sheet_814.cell(row=start_row, column=col)\n",
        "\n",
        "                    # Copy formatting if any\n",
        "                    if source_cell.has_style:\n",
        "                        target_cell.font = copy(source_cell.font)\n",
        "                        target_cell.border = copy(source_cell.border)\n",
        "                        target_cell.fill = source_cell.fill\n",
        "                        target_cell.number_format = source_cell.number_format\n",
        "                        target_cell.protection = source_cell.protection\n",
        "                        target_cell.alignment = source_cell.alignment\n",
        "\n",
        "                # Set row height for larger font\n",
        "                sheet_814.row_dimensions[start_row].height = 90\n",
        "\n",
        "\n",
        "            set_left_font48(sheet_814[f\"E{start_row}\"], \"joined stops\")\n",
        "            set_left_font48(sheet_814[f\"F{start_row}\"], \"joined stops\")\n",
        "            set_left_font48(sheet_814[f\"G{start_row}\"], total_duration)\n",
        "            set_left_font48(sheet_814[f\"H{start_row}\"], location)\n",
        "            set_left_font48(sheet_814[f\"K{start_row}\"], stop_group)\n",
        "            set_left_font48(sheet_814[f\"N{start_row}\"], combined)\n",
        "\n",
        "            start_row += 1\n",
        "\n",
        "        else:\n",
        "            # ‚úÖ true individual stop (no join ID or empty)\n",
        "            if start_row >= 49:\n",
        "                sheet_814.insert_rows(start_row)\n",
        "\n",
        "                # üßΩ Unmerge any cells that were auto-merged in the inserted row\n",
        "                for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "                    if merged_range.min_row == start_row:\n",
        "                        try:\n",
        "                            sheet_814.unmerge_cells(str(merged_range))\n",
        "                        except KeyError:\n",
        "                            pass # merged cell may already have been cleared\n",
        "\n",
        "            # üé® Copy style from row 49 to the new row\n",
        "                for col in range(1, sheet_814.max_column + 1):\n",
        "                    source_cell = sheet_814.cell(row=49, column=col)\n",
        "                    target_cell = sheet_814.cell(row=start_row, column=col)\n",
        "\n",
        "                    # Copy formatting if any\n",
        "                    if source_cell.has_style:\n",
        "                        target_cell.font = copy(source_cell.font)\n",
        "                        target_cell.border = copy(source_cell.border)\n",
        "                        target_cell.fill = copy(source_cell.fill)\n",
        "                        target_cell.number_format = copy(source_cell.number_format)\n",
        "                        target_cell.protection = copy(source_cell.protection)\n",
        "                        target_cell.alignment = copy(source_cell.alignment)\n",
        "\n",
        "                # Set row height for larger font\n",
        "                sheet_814.row_dimensions[start_row].height = 90\n",
        "\n",
        "            start_time = row['start'].strftime('%H:%M') if pd.notna(row['start']) else ''\n",
        "            end_time = row['end'].strftime('%H:%M') if pd.notna(row['end']) else ''\n",
        "            duration = ''\n",
        "            if pd.notna(row['start']) and pd.notna(row['end']):\n",
        "                duration = (row['end'] - row['start']).total_seconds() / 60\n",
        "                duration = round(duration, 0)\n",
        "\n",
        "            location_val = str(row.get('location', '')).strip()\n",
        "            location = dict_locations.get(location_val, location_val)\n",
        "            stop_group_val = str(row.get('stopGroup', '')).strip()\n",
        "            stop_group = dict_stop_type.get(stop_group_val, stop_group_val)\n",
        "            stop_val = str(row.get('stop', '')).strip()\n",
        "            stop = dict_stop_reasons.get(stop_val, stop_val)\n",
        "            comment = str(row.get('comment', '')).strip()\n",
        "            combined = f\"{stop} ({comment})\".strip()\n",
        "\n",
        "            set_left_font48(sheet_814[f\"E{start_row}\"], start_time)\n",
        "            set_left_font48(sheet_814[f\"F{start_row}\"], end_time)\n",
        "            set_left_font48(sheet_814[f\"G{start_row}\"], duration)\n",
        "            set_left_font48(sheet_814[f\"H{start_row}\"], location)\n",
        "            set_left_font48(sheet_814[f\"K{start_row}\"], stop_group)\n",
        "            set_left_font48(sheet_814[f\"N{start_row}\"], combined)\n",
        "\n",
        "            start_row += 1\n",
        "\n",
        "    # üîÅ Speed losses API\n",
        "    speed_url = f\"https://api.evocon.com/api/reports/speedlosses_json?stationId={station_id}&startTime=2025-03-01&endTime=2050-01-01\"\n",
        "    response = requests.get(speed_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Speed losses API error: {response.status_code}\")\n",
        "\n",
        "    speed_df = pd.DataFrame(response.json())\n",
        "    speed_df['start'] = pd.to_datetime(speed_df['start'], errors='coerce')\n",
        "    speed_df['shiftName'] = speed_df['shiftName'].astype(str).str.strip()\n",
        "    speed_df['performanceLossNotes'] = speed_df['performanceLossNotes'].astype(str).fillna('').str.strip()\n",
        "    speed_df['stopMinutes'] = pd.to_numeric(speed_df['stopMinutes'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Filter by selected date and shift\n",
        "    if shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        speed_filtered = speed_df[\n",
        "            ((speed_df['start'].dt.date == selected_date) & (speed_df['start'].dt.time > time(21, 30))) |\n",
        "            ((speed_df['start'].dt.date == next_day) & (speed_df['start'].dt.time <= time(5, 30)))\n",
        "        ]\n",
        "    else:\n",
        "        speed_filtered = speed_df[\n",
        "            (speed_df['start'].dt.date == selected_date) &\n",
        "            (speed_df['shiftName'] == shift_label)\n",
        "        ]\n",
        "\n",
        "    # Group by comment (performanceLossNotes)\n",
        "    grouped_speed = speed_filtered.groupby('performanceLossNotes')\n",
        "    for comment_val, group in grouped_speed:\n",
        "      total_duration = round(group['stopMinutes'].sum(), 2)\n",
        "      comment = dict_speed_loss_reasons.get(comment_val, comment_val)\n",
        "\n",
        "\n",
        "      if start_row >= 49:\n",
        "        sheet_814.insert_rows(start_row)\n",
        "        for merged_range in list(sheet_814.merged_cells.ranges):\n",
        "            if merged_range.min_row == start_row:\n",
        "                try:\n",
        "                    sheet_814.unmerge_cells(str(merged_range))\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        # Copy style from row 49 to the new row, including row height\n",
        "        copy_row_format(sheet_814, 40, start_row)\n",
        "\n",
        "\n",
        "    # Write values to sheet\n",
        "      set_left(sheet_814[f\"E{start_row}\"], \"gubitak brzine\")\n",
        "      set_left(sheet_814[f\"F{start_row}\"], \"gubitak brzine\")\n",
        "      set_left(sheet_814[f\"G{start_row}\"], total_duration)\n",
        "      set_left(sheet_814[f\"K{start_row}\"], dict_speed_loss_group.get(\"Speed Loss\", \"Speed Loss\")) # Always \"Speed Loss\" group\n",
        "      set_left(sheet_814[f\"N{start_row}\"], comment)\n",
        "\n",
        "      start_row += 1\n",
        "\n",
        "\n",
        "\n",
        "    # üöÄ STEP: Fill product info (product name, SKU, start/end time of runs) into Sheet1\n",
        "\n",
        "\n",
        "    # üß© 1. Get shift time boundaries\n",
        "    shift_start_time, shift_end_time = get_shift_bounds(shift, selected_date)\n",
        "\n",
        "    # üëÄ 2. Get first product info from the first stop\n",
        "    first_stop_row = filtered_losses.iloc[0]\n",
        "    first_product_name = str(first_stop_row.get(\"productName\", \"\")).strip()\n",
        "    first_product_sku = str(first_stop_row.get(\"productSku\", \"\")).strip()\n",
        "\n",
        "    # üñãÔ∏è Fill F10 (SKU), F11 (product), and J10 (shift start time) initially\n",
        "    sheet_814[\"F10\"].value = first_product_sku\n",
        "    sheet_814[\"F11\"].value = first_product_name\n",
        "    sheet_814[\"J10\"].value = shift_start_time.strftime(\"%H:%M\")\n",
        "\n",
        "    # ‚è≥ 3. Fetch production runs and filter those within this shift\n",
        "    prod_url = f\"https://api.evocon.com/api/reports/production_runs_json?startTime=2025-03-01&endTime=2050-01-01&stationId={station_id}\"\n",
        "    prod_df = pd.DataFrame(requests.get(prod_url, headers=headers).json())\n",
        "\n",
        "    # Normalize times and dates\n",
        "    prod_df['startDate'] = pd.to_datetime(prod_df['startDate'], errors='coerce').dt.date\n",
        "    prod_df['startTime'] = pd.to_datetime(prod_df['startTime'], errors='coerce')\n",
        "    prod_df['endTime'] = pd.to_datetime(prod_df['endTime'], errors='coerce')\n",
        "\n",
        "    # For 3rd shift, we must span dates\n",
        "    if shift == 3:\n",
        "        next_day = selected_date + pd.Timedelta(days=1)\n",
        "        shift_filter = (\n",
        "            ((prod_df['startDate'] == selected_date) & (prod_df['startTime'].dt.time >= shift_start_time)) |\n",
        "            ((prod_df['startDate'] == next_day) & (prod_df['startTime'].dt.time <= shift_end_time))\n",
        "        )\n",
        "    else:\n",
        "        shift_filter = (\n",
        "            (prod_df['startDate'] == selected_date) &\n",
        "            (prod_df['startTime'].dt.time >= shift_start_time) &\n",
        "            (prod_df['startTime'].dt.time <= shift_end_time)\n",
        "        )\n",
        "\n",
        "    prod_filtered = prod_df[shift_filter].sort_values(by=\"startTime\").reset_index(drop=True)\n",
        "\n",
        "    # üß† 4. Build initial production run list from stop data\n",
        "    production_runs = [{\n",
        "        \"sku\": first_product_sku,\n",
        "        \"product\": first_product_name,\n",
        "        \"start_time\": shift_start_time.strftime(\"%H:%M\"),\n",
        "        \"end_time\": None  # to be filled later\n",
        "    }]\n",
        "\n",
        "    # üîç 5. Detect \"Changeover and CIP\" stops to determine end/start transitions\n",
        "    changeover_rows = filtered_losses[filtered_losses[\"stopGroup\"] == \"Changeover and CIP\"].copy()\n",
        "    changeover_rows = changeover_rows.sort_values(by=\"start\").reset_index(drop=True)\n",
        "\n",
        "    # Assign end time of first run\n",
        "    if not changeover_rows.empty and pd.notna(changeover_rows.iloc[0][\"start\"]):\n",
        "        production_runs[0][\"end_time\"] = changeover_rows.iloc[0][\"start\"].strftime(\"%H:%M\")\n",
        "    else:\n",
        "        production_runs[0][\"end_time\"] = shift_end_time.strftime(\"%H:%M\")\n",
        "\n",
        "\n",
        "    # üîÅ 6. Append production runs from prod_filtered (with proper changeover sequencing)\n",
        "    changeover_rows = changeover_rows.reset_index(drop=True)  # Safe for iloc\n",
        "    all_stops = filtered_losses.reset_index(drop=True)        # Ensure same for full stops list\n",
        "\n",
        "    changeover_idx = 0\n",
        "    last_valid_end = None\n",
        "\n",
        "    for i, row in prod_filtered.iterrows():\n",
        "        sku = str(row.get(\"sku\", \"\")).strip()\n",
        "        product = str(row.get(\"product\", \"\")).strip()\n",
        "\n",
        "        # üß© Find the last \"Changeover and CIP\" in a consecutive sequence\n",
        "        cip_end_time = None\n",
        "        while changeover_idx < len(changeover_rows):\n",
        "            current_cip = changeover_rows.iloc[changeover_idx]\n",
        "            changeover_idx += 1\n",
        "\n",
        "            # If last or next stop is not \"Changeover and CIP\", this is the last in sequence\n",
        "            if changeover_idx == len(changeover_rows):\n",
        "                cip_end_time = current_cip.get(\"end\")\n",
        "                break\n",
        "\n",
        "            next_cip = changeover_rows.iloc[changeover_idx]\n",
        "            curr_idx_in_all = all_stops[all_stops['start'] == current_cip['start']].index[0]\n",
        "\n",
        "            if curr_idx_in_all + 1 < len(all_stops):\n",
        "                next_stop_group = all_stops.iloc[curr_idx_in_all + 1].get(\"stopGroup\", \"\")\n",
        "                if next_stop_group != \"Changeover and CIP\":\n",
        "                    cip_end_time = current_cip.get(\"end\")\n",
        "                    break\n",
        "\n",
        "        # üïí Start time = end of last CIP, or start of shift\n",
        "        if pd.notna(cip_end_time):\n",
        "            start_time = cip_end_time\n",
        "        else:\n",
        "            start_time = row.get(\"startTime\") or shift_start_time\n",
        "\n",
        "        start_str = start_time.strftime(\"%H:%M\") if pd.notna(start_time) else \"\"\n",
        "\n",
        "        # üïõ End time = next CIP start, or shift end\n",
        "        if changeover_idx < len(changeover_rows):\n",
        "            next_cip_start = changeover_rows.iloc[changeover_idx][\"start\"]\n",
        "            end_str = next_cip_start.strftime(\"%H:%M\") if pd.notna(next_cip_start) else shift_end_time.strftime(\"%H:%M\")\n",
        "        else:\n",
        "            end_str = shift_end_time.strftime(\"%H:%M\")\n",
        "\n",
        "        production_runs.append({\n",
        "            \"sku\": sku,\n",
        "            \"product\": product,\n",
        "            \"start_time\": start_str,\n",
        "            \"end_time\": end_str\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #version of insert rows and writing with font and size formatting and with row height adjustment\n",
        "\n",
        "\n",
        "    # üî† Font config\n",
        "    prod_font = Font(name=\"Arial\", size=48)\n",
        "    bold_font = Font(bold=True)\n",
        "\n",
        "    # üß± Insert new rows (if more than one product)\n",
        "    insert_point = 10\n",
        "    extra_runs = len(production_runs) - 1\n",
        "\n",
        "    if extra_runs > 0:\n",
        "        for _ in range(extra_runs * 2):\n",
        "            sheet_814.insert_rows(insert_point)\n",
        "\n",
        "    # üßæ content to be written in new cells inserted\n",
        "    title_map = {\n",
        "        \"E\": \"≈†ifra:\",\n",
        "        \"E_lower\": \"Naziv proizvoda:\",\n",
        "        \"H\": \"Proizvodnja od (prva boca kontinuirano na punjaƒçu):\",\n",
        "        \"H_lower\": \"Proizvodnja do (zadnja boca na punjaƒçu):\",\n",
        "    }\n",
        "\n",
        "    # üß¨ Apply values and formatting row by row\n",
        "    for i, run in enumerate(production_runs):\n",
        "        row_top = 10 + (i * 2)\n",
        "        row_bottom = row_top + 1\n",
        "\n",
        "        # Copy titles to column E and H\n",
        "        sheet_814[f\"E{row_top}\"].value = title_map[\"E\"]\n",
        "        sheet_814[f\"E{row_bottom}\"].value = title_map[\"E_lower\"]\n",
        "        sheet_814[f\"H{row_top}\"].value = title_map[\"H\"]\n",
        "        sheet_814[f\"H{row_bottom}\"].value = title_map[\"H_lower\"]\n",
        "\n",
        "        for col in ['E', 'F', 'H', 'J']:\n",
        "            for r in [row_top, row_bottom]:\n",
        "                cell = sheet_814[f\"{col}{r}\"]\n",
        "                cell.font = prod_font\n",
        "                cell.alignment = Alignment(horizontal='left', wrap_text=True)\n",
        "\n",
        "        # Fill values for product info\n",
        "        set_left(sheet_814[f\"F{row_top}\"], run[\"sku\"])\n",
        "        set_left(sheet_814[f\"F{row_bottom}\"], run[\"product\"])\n",
        "        set_left(sheet_814[f\"J{row_top}\"], run[\"start_time\"])\n",
        "        set_left(sheet_814[f\"J{row_bottom}\"], run[\"end_time\"])\n",
        "        # ovdje je u og kodu bilo height = 159\n",
        "\n",
        "    # Adjust row height for readability\n",
        "        sheet_814.row_dimensions[row_top].height = 159\n",
        "        sheet_814.row_dimensions[row_bottom].height = 159\n",
        "    \"\"\"\n",
        "        # Dynamically adjust row height based on the maximum number of lines in the cells\n",
        "        def get_max_lines(row_num, columns):\n",
        "            max_lines = 1\n",
        "            for col in columns:\n",
        "                cell_value = str(sheet_814[f\"{col}{row_num}\"].value or \"\")\n",
        "                lines = cell_value.count('\\n') + 1\n",
        "                max_lines = max(max_lines, lines)\n",
        "            return max_lines\n",
        "\n",
        "        font_size = prod_font.size if hasattr(prod_font, 'size') else 12\n",
        "        line_height = font_size * 1.2  # Approximate line height\n",
        "        for r in [row_top, row_bottom]:\n",
        "            max_lines = get_max_lines(r, ['E', 'F', 'H', 'J'])\n",
        "            sheet_814.row_dimensions[r].height = max(30, int(line_height * max_lines))\n",
        "    \"\"\"\n",
        "    # Automatically adjust row height to fit all text\n",
        "    #for r in [row_top, row_bottom]:\n",
        "     #   max_height = 15  # minimum height\n",
        "      #  for col in ['E', 'F', 'H', 'J']:\n",
        "       #     cell = sheet_814[f\"{col}{r}\"]\n",
        "        #    if cell.value:\n",
        "                # Estimate height: 1.2x font size per line, count lines by splitting on '\\n'\n",
        "         #       lines = str(cell.value).split('\\n')\n",
        "          #      num_lines = max(len(lines), 1)\n",
        "           ##    max_height = max(max_height, height)\n",
        "        #sheet_814.row_dimensions[r].height = max_height\n",
        "\n",
        "\n",
        "    #footer = 6 #doljnja sastavnica\n",
        "\n",
        "    def make_header(from_sheet, header_rows):\n",
        "    #from_sheet se samo referira na sheet excela\n",
        "    # postavljanje headera kao rows\n",
        "        from_sheet.print_title_rows = f'1:{header_rows}'\n",
        "\n",
        "    header = 30 #gornja sastavnica\n",
        "\n",
        "# mozda bi radilo da u taj length dodam AND raspon footera al idk bi li slomilo kod\n",
        "\n",
        "    def make_footer(from_sheet, footer_rows):\n",
        "        max_row = from_sheet.max_row\n",
        "        footer_texts = []\n",
        "        for row in range(max_row - footer_rows, max_row + 1):\n",
        "            row_values = []\n",
        "            for col in range(1, from_sheet.max_column + 1):\n",
        "                cell = from_sheet.cell(row=row, column=col)\n",
        "                if cell.value:\n",
        "                    row_values.append(str(cell.value))\n",
        "            if row_values:\n",
        "                footer_texts.append(\" | \".join(row_values))\n",
        "        # pretvori sve u string jer excel supporta samo string footers iz nekog razloga\n",
        "        footer_str = \"\\n\".join(footer_texts)\n",
        "        # Set footer font explicitly to Arial, size 48\n",
        "        font_str = '&\"Arial\"&48'\n",
        "        formatted_footer = f'{font_str}{footer_str}'\n",
        "        from_sheet.oddFooter.center.text = formatted_footer\n",
        "\n",
        "    make_header(sheet_814, header)\n",
        "    # make_footer(sheet_814, footer)\n",
        "\n",
        "    # üíæ Step 11: Save and download\n",
        "    from openpyxl.utils import get_column_letter\n",
        "\n",
        "# namjesti print area na cijeli range\n",
        "# jer ako netko ne klikne \"ignore print range\" pri printanju\n",
        "# nece iskopirati sve redove\n",
        "    max_row = sheet_814.max_row\n",
        "    max_col = sheet_814.max_column\n",
        "    print_area = f\"A1:{get_column_letter(max_col)}{max_row}\"\n",
        "    sheet_814.print_area = print_area\n",
        "\n",
        "\n",
        "    #output_file = output_dir / f\"updated_shift_report_{line_input}_{date_str}.xlsx\"\n",
        "    #wb.save(output_file)\n",
        "\n",
        "    # Define the output directory\n",
        "    #output_dir = \"excel_reports\"\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    #if not os.path.exists(output_dir):\n",
        "    #  os.makedirs(output_dir)\n",
        "\n",
        "    # Save the Excel file\n",
        "    #output_file = f\"{output_dir}/updated_shift_report_{line_input}_{date_str}.xlsx\"\n",
        "    #wb.save(output_file)\n",
        "\n",
        "    output_file = f\"{line_input}_{shift}_{date_str}.xlsx\"\n",
        "    wb.save(output_file)\n",
        "    # Create the folder (if it doesn't exist)\n",
        "    os.makedirs(\"excel_reports\", exist_ok=True)\n",
        "\n",
        "    # Move the file to the output directory\n",
        "    os.replace(output_file, os.path.join(\"excel_reports\", output_file))\n",
        "\n",
        "    #output_file = \"updated_shift_report.xlsx\"\n",
        "    #wb.save(output_file)\n",
        "    #files.download(output_file)\n",
        "    #wb.save(full_path)\n",
        "    #wb.save(output_file)\n",
        "    #files.download(output_file)\n",
        "    \"\"\"\n",
        "        # Git commit and push\n",
        "    try:\n",
        "      print(\"---GIT ADD---\")\n",
        "      subprocess.run([\"git\", \"add\", str(full_path)], check=True, capture_output=True, text=True)\n",
        "      print(\"---GIT COMMIT---\")\n",
        "      subprocess.run([\"git\", \"commit\", \"-m\", f\"Add Excel file {filename}\"], check=True, capture_output=True, text=True)\n",
        "      print(\"---GIT PUSH---\")\n",
        "      subprocess.run([\"git\", \"push\"], check=True, capture_output=True, text=True)\n",
        "      print(\"‚úÖ Excel file committed and pushed to GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "      print(f\"‚ùå Git operation failed: {e}\")\n",
        "      print(f\"---STDOUT---:\\n{e.stdout}\")\n",
        "      print(f\"---STDERR---:\\n{e.stderr}\")\n",
        "    \"\"\""
      ]
    }
  ]
}